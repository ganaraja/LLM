{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6da7f8a",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://supportvectors.ai/logo-poster-transparent.png\" width=\"400px\" style=\"opacity:0.7\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6023be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!-- Many of the styles here are inspired by: \n",
       "    https://towardsdatascience.com/10-practical-tips-you-need-to-know-to-personalize-jupyter-notebook-fbd202777e20 \n",
       "\n",
       "\n",
       "    On the author's local machine, these exist in the custom.css file. However, in order to keep uniform look and feel, \n",
       "    and at the request of participants, I have added it to this common import-file here.\n",
       "\n",
       "    -->\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Lora:400,700|Montserrat:300\" rel=\"stylesheet\">\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Crimson+Pro&family=Literata&display=swap\" rel=\"stylesheet\">\n",
       "<style>\n",
       "\n",
       "\n",
       "#ipython_notebook::before{\n",
       " content:\"Neural Architectures\";\n",
       "        color: white;\n",
       "        font-weight: bold;\n",
       "        text-transform: uppercase;\n",
       "        font-family: 'Lora',serif;\n",
       "        font-size:16pt;\n",
       "        margin-bottom:15px;\n",
       "        margin-top:15px;\n",
       "\n",
       "}\n",
       "body > #header {\n",
       "    #background: #D15555;\n",
       "    background: linear-gradient(to bottom, indianred 0%, #fff 100%);\n",
       "    opacity: 0.8;\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       ".navbar-default .navbar-nav > li > a, #kernel_indicator {\n",
       "    color: white;\n",
       "    transition: all 0.25s;\n",
       "    font-size:10pt;\n",
       "    font-family: sans-serif;\n",
       "    font-weight:normal;\n",
       "}\n",
       ".navbar-default {\n",
       "    padding-left:100px;\n",
       "    background: none;\n",
       "    border: none;\n",
       "}\n",
       "\n",
       "\n",
       "body > menubar-container {\n",
       "    background-color: wheat;\n",
       "}\n",
       "#ipython_notebook img{                                                                                        \n",
       "    display:block; \n",
       "\n",
       "    background: url(\"https://www.supportvectors.com/wp-content/uploads/2016/03/logo-poster-smaller.png\") no-repeat;\n",
       "    background-size: contain;\n",
       "\n",
       "    padding-left: 600px;\n",
       "    padding-right: 100px;\n",
       "\n",
       "    -moz-box-sizing: border-box;\n",
       "    box-sizing: border-box;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "body {\n",
       " #font-family:  'Literata', serif;\n",
       "    font-family:'Lora', san-serif;\n",
       "    text-align: justify;\n",
       "    font-weight: 400;\n",
       "    font-size: 12pt;\n",
       "}\n",
       "\n",
       "iframe{\n",
       "    width:100%;\n",
       "    min-height:600px;\n",
       "}\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "# font-family: 'Montserrat', sans-serif;\n",
       " font-family:'Lora', serif;\n",
       " font-weight: 200;\n",
       " text-transform: uppercase;\n",
       " color: #EC7063 ;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    color: #000080;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       "#notebook_name {\n",
       "    font-weight: 600;\n",
       "    font-size:20pt;\n",
       "    text-variant:uppercase;\n",
       "    color: wheat; \n",
       "    margin-right:20px;\n",
       "    margin-left:-500px;\n",
       "}\n",
       "#notebook_name:hover {\n",
       "background-color: salmon;\n",
       "}\n",
       "\n",
       "\n",
       ".dataframe { /* dataframe atau table */\n",
       "    background: white;\n",
       "    box-shadow: 0px 1px 2px #bbb;\n",
       "}\n",
       ".dataframe thead th, .dataframe tbody td {\n",
       "    text-align: center;\n",
       "    padding: 1em;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       ".output {\n",
       "    align-items: center;\n",
       "}\n",
       "\n",
       "div.cell {\n",
       "    transition: all 0.25s;\n",
       "    border: none;\n",
       "    position: relative;\n",
       "    top: 0;\n",
       "}\n",
       "div.cell.selected, div.cell.selected.jupyter-soft-selected {\n",
       "    border: none;\n",
       "    background: transparent;\n",
       "    box-shadow: 0 6px 18px #aaa;\n",
       "    z-index: 10;\n",
       "    top: -10px;\n",
       "}\n",
       ".CodeMirror pre, .CodeMirror-dialog, .CodeMirror-dialog .CodeMirror-search-field, .terminal-app .terminal {\n",
       "    font-family: 'Hack' , serif; \n",
       "    font-weight: 500;\n",
       "    font-size: 14pt;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "</style>    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<div style=\"color:#aaa;font-size:8pt\">\n",
       "<hr/>\n",
       "&copy; SupportVectors. All rights reserved. <blockquote>This notebook is the intellectual property of SupportVectors, and part of its training material. \n",
       "Only the participants in SupportVectors workshops are allowed to study the notebooks for educational purposes currently, but is prohibited from copying or using it for any other purposes without written permission.\n",
       "\n",
       "<b> These notebooks are chapters and sections from Asif Qamar's textbook that he is writing on Data Science. So we request you to not circulate the material to others.</b>\n",
       " </blockquote>\n",
       " <hr/>\n",
       "</div>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3987fac2",
   "metadata": {},
   "source": [
    "\n",
    "# Vector Indices and Approximate Nearest Neighbor Search\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Vector Indices](#introduction-to-vector-indices)\n",
    "2. [Why Vector Indices?](#why-vector-indices)\n",
    "3. [Approximate Nearest Neighbor (ANN) Search](#approximate-nearest-neighbor-ann-search)\n",
    "4. [ANN Search Algorithms](#ann-search-algorithms)\n",
    "5. [FAISS: Facebook AI Similarity Search](#faiss-facebook-ai-similarity-search)\n",
    "6. [Vector Databases](#vector-databases)\n",
    "7. [Qdrant: A Modern Vector Database](#qdrant-a-modern-vector-database)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a22dc1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Introduction to Vector Indices\n",
    "\n",
    "Vector indices are specialized data structures designed to efficiently store, organize, and retrieve high-dimensional vector representations of data. In the era of deep learning and AI, data is increasingly represented as dense vectors (embeddings) that capture semantic meaning in a continuous vector space.\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "Embeddings are dense vector representations that map discrete objects (text, images, audio, video) to points in a high-dimensional vector space. The key insight is that semantically similar objects are mapped to nearby points in this space.\n",
    "\n",
    "**Mathematical Representation:**\n",
    "Given an object $x$ and an embedding function $f$, we have:\n",
    "$$f: \\mathcal{X} \\rightarrow \\mathbb{R}^d$$\n",
    "where $d$ is the dimensionality of the embedding space.\n",
    "\n",
    "### Types of Embeddings\n",
    "\n",
    "1. **Text Embeddings**: Generated by models like BERT, GPT, or sentence transformers\n",
    "2. **Image Embeddings**: Produced by CNNs, Vision Transformers, or CLIP\n",
    "3. **Audio Embeddings**: Created by models like Wav2Vec, Whisper\n",
    "4. **Video Embeddings**: Generated by temporal models or frame aggregation\n",
    "5. **Multimodal Embeddings**: Combining multiple modalities (e.g., CLIP for text-image pairs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc2756",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Why Vector Indices?\n",
    "\n",
    "As the dimensionality of vectors increases, traditional search methods become increasingly inefficient. \n",
    "\n",
    "**The Problem:**\n",
    "- Linear search through $N$ vectors: $O(N)$ time complexity\n",
    "- For high-dimensional spaces, distance calculations become expensive\n",
    "- Traditional indexing methods (B-trees, hash tables) don't work well for similarity search\n",
    "\n",
    "Traditional methods are optimized for:\n",
    "- Exact matches\n",
    "- Range queries\n",
    "- Structured data\n",
    "\n",
    "They are **not** optimized for:\n",
    "- Similarity search\n",
    "- High-dimensional vectors\n",
    "- Approximate matching\n",
    "\n",
    "Vector indices address these challenges by:\n",
    "1. **Dimensionality Reduction**: Reducing the effective dimensionality\n",
    "2. **Space Partitioning**: Dividing the vector space into manageable regions\n",
    "3. **Approximation**: Trading exactness for speed\n",
    "4. **Specialized Data Structures**: Optimized for vector operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7a78d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Approximate Nearest Neighbor (ANN) Search\n",
    "\n",
    "### Problem Definition\n",
    "\n",
    "Given a query vector $q \\in \\mathbb{R}^d$ and a set of vectors $S = \\{v_1, v_2, ..., v_n\\}$, find the $k$ vectors in $S$ that are most similar to $q$.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "$$\\text{ANN}(q, S, k) = \\arg\\min_{v \\in S} \\text{dist}(q, v)$$\n",
    "\n",
    "where $\\text{dist}$ is a distance metric (typically L2, cosine, or dot product). The function returns the $k$ vectors from $S$ that minimize the distance to $q$.\n",
    "\n",
    "### Exact vs. Approximate Search\n",
    "\n",
    "| Aspect | Exact Search | Approximate Search |\n",
    "|--------|--------------|-------------------|\n",
    "| **Accuracy** | 100% | 90-99% |\n",
    "| **Speed** | Slow | Fast |\n",
    "| **Memory** | High | Lower |\n",
    "| **Scalability** | Poor | Excellent |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c181c02",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ANN Search Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b13f5c",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "LSH is a technique that hashes input items so that similar items map to the same \"buckets\" with high probability.\n",
    "\n",
    "**Core Idea:**\n",
    "- Use hash functions that preserve similarity\n",
    "- Similar vectors are likely to hash to the same bucket\n",
    "- Search only within the bucket of the query\n",
    "\n",
    "**Pros:**\n",
    "- Simple to implement\n",
    "- Good theoretical guarantees\n",
    "- Works well for high-dimensional data\n",
    "\n",
    "**Cons:**\n",
    "- Requires multiple hash tables for good recall\n",
    "- Memory overhead\n",
    "- Not optimal for very large datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50020dfe",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Hierarchical Navigable Small World (HNSW)\n",
    "\n",
    "HNSW constructs a hierarchical graph structure where each level contains a subset of the previous level, creating a \"small world\" network that enables efficient navigation through the vector space.\n",
    "\n",
    "**Core Idea:**\n",
    "- Build a multi-layer graph where each layer is a subset of the layer below\n",
    "- Higher layers are sparser and enable fast \"long-range\" navigation\n",
    "- Lower layers are denser and provide precise local search\n",
    "- Navigate from top to bottom using greedy search, starting with coarse approximation and refining\n",
    "\n",
    "**How HNSW Works:**\n",
    "\n",
    "1. **Hierarchical Structure:**\n",
    "   - **Top layer (Layer L)**: Very sparse, contains few points, enables fast global navigation\n",
    "   - **Middle layers**: Moderate density, balance between speed and accuracy\n",
    "   - **Bottom layer (Layer 0)**: Dense, contains all points, provides precise local search\n",
    "\n",
    "2. **Graph Construction Process:**\n",
    "   - Each point is assigned to layers probabilistically (higher layers with lower probability)\n",
    "   - For each point, find its nearest neighbors in the current layer\n",
    "   - Connect the point to these neighbors (up to M connections per layer)\n",
    "   - Repeat for all layers the point belongs to\n",
    "\n",
    "3. **Search Process:**\n",
    "   - Start from the top layer with a **designated entry point** (not random - it's the highest-level point in the graph)\n",
    "   - Find the **nearest neighbor to the query** in the current layer using local search\n",
    "   - Use this nearest neighbor as the entry point for the layer below\n",
    "   - Continue until reaching the bottom layer\n",
    "   - Perform local search in the bottom layer to find final results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518411c6",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Inverted File Index (IVF)\n",
    "\n",
    "IVF partitions the vector space into clusters and maintains an inverted index mapping clusters to vectors.\n",
    "\n",
    "**Core Idea:**\n",
    "1. Cluster vectors using k-means\n",
    "2. Create inverted index: cluster_id → list of vectors\n",
    "3. For queries, search only in relevant clusters\n",
    "\n",
    "**Pros:**\n",
    "- Simple and effective\n",
    "- Good for large datasets\n",
    "- Easy to tune\n",
    "\n",
    "**Cons:**\n",
    "- Performance depends on clustering quality\n",
    "- Not optimal for very high-dimensional data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d926b53",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Product Quantization (PQ)\n",
    "\n",
    "PQ reduces memory usage by quantizing vectors into smaller sub-vectors and encoding them.\n",
    "\n",
    "**Core Idea:**\n",
    "1. Split vector into $m$ sub-vectors\n",
    "2. Quantize each sub-vector independently\n",
    "3. Store only the quantization indices\n",
    "\n",
    "**Pros:**\n",
    "- Massive memory reduction\n",
    "- Fast distance computation\n",
    "- Good compression ratio\n",
    "\n",
    "**Cons:**\n",
    "- Loss of precision\n",
    "- Complex implementation\n",
    "- Not suitable for exact search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da240d3d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## FAISS: Facebook AI Similarity Search\n",
    "\n",
    "### Introduction\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors. It was developed by Facebook Research and has become a cornerstone in the ANN search ecosystem.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Multiple Index Types**: Supports various ANN algorithms\n",
    "2. **GPU Acceleration**: CUDA support for faster computation\n",
    "3. **Memory Efficiency**: Optimized for large-scale datasets\n",
    "4. **Flexibility**: Composable index structures\n",
    "\n",
    "### Limitations of FAISS\n",
    "\n",
    "1. **No Persistence**: Indexes must be rebuilt after restart\n",
    "2. **No Metadata**: Can't store additional information with vectors\n",
    "3. **No Updates**: Adding/removing vectors requires rebuilding\n",
    "4. **No Filtering**: Can't filter results based on metadata\n",
    "5. **Complex Configuration**: Many parameters to tune\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf713ed",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Vector Databases\n",
    "\n",
    "### Why Vector Databases?\n",
    "\n",
    "While FAISS is excellent for pure vector search, modern applications often need:\n",
    "\n",
    "1. **Persistence**: Data survives restarts\n",
    "2. **Metadata**: Store additional information with vectors\n",
    "3. **Filtering**: Query by both vector similarity and metadata\n",
    "4. **Updates**: Add/remove vectors without rebuilding\n",
    "5. **Scalability**: Distributed storage and search\n",
    "6. **Real-time**: Handle streaming data\n",
    "\n",
    "### Popular Vector Databases\n",
    "\n",
    "#### 1. Qdrant\n",
    "- **Language**: Rust\n",
    "- **Features**: High performance, rich filtering, real-time updates\n",
    "- **Use Cases**: Production systems, real-time applications\n",
    "\n",
    "#### 2. ChromaDB\n",
    "- **Language**: Python\n",
    "- **Features**: Easy to use, good for prototyping\n",
    "- **Use Cases**: Research, development, small-scale applications\n",
    "\n",
    "#### 3. Pinecone\n",
    "- **Language**: Cloud service\n",
    "- **Features**: Managed service, auto-scaling\n",
    "- **Use Cases**: Production applications, enterprise\n",
    "\n",
    "#### 4. Weaviate\n",
    "- **Language**: Go\n",
    "- **Features**: GraphQL interface, schema management\n",
    "- **Use Cases**: Knowledge graphs, semantic search\n",
    "\n",
    "#### 5. Milvus\n",
    "- **Language**: C++\n",
    "- **Features**: High performance, distributed\n",
    "- **Use Cases**: Large-scale production systems\n",
    "\n",
    "### Comparison Matrix\n",
    "\n",
    "The following comparison is based on published benchmarks and performance studies. Note that performance characteristics can vary significantly based on workload, hardware, and configuration.\n",
    "\n",
    "| Database | Language | Architecture | Key Strengths | Key Limitations | Best Use Cases |\n",
    "|----------|----------|--------------|---------------|-----------------|----------------|\n",
    "| **Qdrant** | Rust | Single-node, distributed | High performance, rich filtering, real-time updates | Limited ecosystem, newer product | Production systems, real-time applications |\n",
    "| **ChromaDB** | Python | Embedded, client-server | Easy setup, good for prototyping | Limited scalability, performance overhead | Research, development, small-scale applications |\n",
    "| **Pinecone** | Cloud | Managed service | Auto-scaling, zero maintenance | Vendor lock-in, cost at scale | Enterprise applications, managed solutions |\n",
    "| **Weaviate** | Go | GraphQL-native | Schema management, knowledge graphs | Learning curve, resource intensive | Knowledge graphs, semantic search |\n",
    "| **Milvus** | C++ | Distributed | High performance, billion-scale | Complex deployment, operational overhead | Large-scale production systems |\n",
    "\n",
    "#### Performance Benchmarks\n",
    "\n",
    "**Note**: The following benchmarks are from published studies and may not reflect current performance. Always conduct your own testing for your specific use case.\n",
    "\n",
    "| Database | Query Latency (ms) | Throughput (QPS) | Memory Usage | Citation |\n",
    "|----------|-------------------|------------------|--------------|----------|\n",
    "| **Qdrant** | 1-5 | 10K-50K | Low | [Qdrant Benchmarks 2023](https://qdrant.tech/benchmarks/) |\n",
    "| **ChromaDB** | 5-20 | 1K-5K | Medium | [ChromaDB Performance](https://docs.trychroma.com/performance) |\n",
    "| **Pinecone** | 10-50 | 1K-10K | N/A (cloud) | [Pinecone Documentation](https://docs.pinecone.io/docs/performance) |\n",
    "| **Weaviate** | 5-15 | 5K-20K | High | [Weaviate Benchmarks](https://weaviate.io/developers/weaviate/benchmarks) |\n",
    "| **Milvus** | 1-10 | 50K-200K | Low | [Milvus Benchmarks](https://milvus.io/docs/benchmark.md) |\n",
    "\n",
    "#### Feature Comparison\n",
    "\n",
    "| Feature | Qdrant | ChromaDB | Pinecone | Weaviate | Milvus |\n",
    "|---------|--------|----------|----------|----------|--------|\n",
    "| **Open Source** | ✅ | ✅ | ❌ | ✅ | ✅ |\n",
    "| **Cloud Managed** | ❌ | ❌ | ✅ | ✅ | ✅ |\n",
    "| **Real-time Updates** | ✅ | ✅ | ✅ | ✅ | ✅ |\n",
    "| **Rich Filtering** | ✅ | ⚠️ | ✅ | ✅ | ✅ |\n",
    "| **Horizontal Scaling** | ✅ | ❌ | ✅ | ✅ | ✅ |\n",
    "| **GPU Support** | ✅ | ❌ | ✅ | ⚠️ | ✅ |\n",
    "| **GraphQL API** | ❌ | ❌ | ❌ | ✅ | ❌ |\n",
    "| **REST API** | ✅ | ✅ | ✅ | ✅ | ✅ |\n",
    "\n",
    "#### Selection Guidelines\n",
    "\n",
    "**Choose Qdrant if:**\n",
    "- You need high performance with rich filtering\n",
    "- You want open-source with production readiness\n",
    "- You have complex query requirements\n",
    "\n",
    "**Choose ChromaDB if:**\n",
    "- You're prototyping or doing research\n",
    "- You need quick setup and easy integration\n",
    "- You have small to medium datasets\n",
    "\n",
    "**Choose Pinecone if:**\n",
    "- You want managed service with auto-scaling\n",
    "- You need enterprise support\n",
    "- You prefer cloud-native solutions\n",
    "\n",
    "**Choose Weaviate if:**\n",
    "- You need knowledge graph capabilities\n",
    "- You want GraphQL-native interface\n",
    "- You're building semantic search applications\n",
    "\n",
    "**Choose Milvus if:**\n",
    "- You need billion-scale vector search\n",
    "- You have the infrastructure for complex deployments\n",
    "- You require maximum performance\n",
    "\n",
    "*Sources: [Vector Database Benchmark 2023](https://zilliz.com/comparison), [Qdrant Benchmarks](https://qdrant.tech/benchmarks/), [Milvus Performance Guide](https://milvus.io/docs/performance_faq.md)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17cdfb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Qdrant: A Modern Vector Database\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Qdrant is a high-performance vector database written in Rust, designed for production use cases requiring real-time vector search with rich filtering capabilities.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **High Performance**: Rust implementation for speed and memory safety\n",
    "2. **Rich Filtering**: Complex boolean expressions on metadata\n",
    "3. **Real-time Updates**: Add/remove vectors without downtime\n",
    "4. **Horizontal Scaling**: Distributed architecture\n",
    "5. **Multiple Distance Metrics**: L2, cosine, dot product\n",
    "6. **Payload Management**: Flexible metadata storage\n",
    "\n",
    "### Architecture\n",
    "\n",
    "#### Core Components\n",
    "1. **Storage Engine**: RocksDB for persistence\n",
    "2. **Vector Index**: HNSW or IVF for ANN search\n",
    "3. **Payload Index**: B-tree for metadata filtering\n",
    "4. **API Layer**: REST and gRPC interfaces\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
