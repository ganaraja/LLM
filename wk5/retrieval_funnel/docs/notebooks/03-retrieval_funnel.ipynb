{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e947289",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://supportvectors.ai/logo-poster-transparent.png\" width=400px style=\"opacity:0.8\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb93a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!-- Many of the styles here are inspired by: \n",
       "    https://towardsdatascience.com/10-practical-tips-you-need-to-know-to-personalize-jupyter-notebook-fbd202777e20 \n",
       "\n",
       "\n",
       "    On the author's local machine, these exist in the custom.css file. However, in order to keep uniform look and feel, \n",
       "    and at the request of participants, I have added it to this common import-file here.\n",
       "\n",
       "    -->\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Lora:400,700|Montserrat:300\" rel=\"stylesheet\">\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Crimson+Pro&family=Literata&display=swap\" rel=\"stylesheet\">\n",
       "<style>\n",
       "\n",
       "\n",
       "#ipython_notebook::before{\n",
       " content:\"Neural Architectures\";\n",
       "        color: white;\n",
       "        font-weight: bold;\n",
       "        text-transform: uppercase;\n",
       "        font-family: 'Lora',serif;\n",
       "        font-size:16pt;\n",
       "        margin-bottom:15px;\n",
       "        margin-top:15px;\n",
       "\n",
       "}\n",
       "body > #header {\n",
       "    #background: #D15555;\n",
       "    background: linear-gradient(to bottom, indianred 0%, #fff 100%);\n",
       "    opacity: 0.8;\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       ".navbar-default .navbar-nav > li > a, #kernel_indicator {\n",
       "    color: white;\n",
       "    transition: all 0.25s;\n",
       "    font-size:10pt;\n",
       "    font-family: sans-serif;\n",
       "    font-weight:normal;\n",
       "}\n",
       ".navbar-default {\n",
       "    padding-left:100px;\n",
       "    background: none;\n",
       "    border: none;\n",
       "}\n",
       "\n",
       "\n",
       "body > menubar-container {\n",
       "    background-color: wheat;\n",
       "}\n",
       "#ipython_notebook img{                                                                                        \n",
       "    display:block; \n",
       "\n",
       "    background: url(\"https://www.supportvectors.com/wp-content/uploads/2016/03/logo-poster-smaller.png\") no-repeat;\n",
       "    background-size: contain;\n",
       "\n",
       "    padding-left: 600px;\n",
       "    padding-right: 100px;\n",
       "\n",
       "    -moz-box-sizing: border-box;\n",
       "    box-sizing: border-box;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "body {\n",
       " #font-family:  'Literata', serif;\n",
       "    font-family:'Lora', san-serif;\n",
       "    text-align: justify;\n",
       "    font-weight: 400;\n",
       "    font-size: 12pt;\n",
       "}\n",
       "\n",
       "iframe{\n",
       "    width:100%;\n",
       "    min-height:600px;\n",
       "}\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "# font-family: 'Montserrat', sans-serif;\n",
       " font-family:'Lora', serif;\n",
       " font-weight: 200;\n",
       " text-transform: uppercase;\n",
       " color: #EC7063 ;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    color: #000080;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       "#notebook_name {\n",
       "    font-weight: 600;\n",
       "    font-size:20pt;\n",
       "    text-variant:uppercase;\n",
       "    color: wheat; \n",
       "    margin-right:20px;\n",
       "    margin-left:-500px;\n",
       "}\n",
       "#notebook_name:hover {\n",
       "background-color: salmon;\n",
       "}\n",
       "\n",
       "\n",
       ".dataframe { /* dataframe atau table */\n",
       "    background: white;\n",
       "    box-shadow: 0px 1px 2px #bbb;\n",
       "}\n",
       ".dataframe thead th, .dataframe tbody td {\n",
       "    text-align: center;\n",
       "    padding: 1em;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       ".output {\n",
       "    align-items: center;\n",
       "}\n",
       "\n",
       "div.cell {\n",
       "    transition: all 0.25s;\n",
       "    border: none;\n",
       "    position: relative;\n",
       "    top: 0;\n",
       "}\n",
       "div.cell.selected, div.cell.selected.jupyter-soft-selected {\n",
       "    border: none;\n",
       "    background: transparent;\n",
       "    box-shadow: 0 6px 18px #aaa;\n",
       "    z-index: 10;\n",
       "    top: -10px;\n",
       "}\n",
       ".CodeMirror pre, .CodeMirror-dialog, .CodeMirror-dialog .CodeMirror-search-field, .terminal-app .terminal {\n",
       "    font-family: 'Hack' , serif; \n",
       "    font-weight: 500;\n",
       "    font-size: 14pt;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "</style>    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<div style=\"color:#aaa;font-size:8pt\">\n",
       "<hr/>\n",
       "&copy; SupportVectors. All rights reserved. <blockquote>This notebook is the intellectual property of SupportVectors, and part of its training material. \n",
       "Only the participants in SupportVectors workshops are allowed to study the notebooks for educational purposes currently, but is prohibited from copying or using it for any other purposes without written permission.\n",
       "\n",
       "<b> These notebooks are chapters and sections from Asif Qamar's textbook that he is writing on Data Science. So we request you to not circulate the material to others.</b>\n",
       " </blockquote>\n",
       " <hr/>\n",
       "</div>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487ea7d9",
   "metadata": {},
   "source": [
    "# Combined Multi-Embedding Retrieval Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This pipeline demonstrates a sophisticated retrieval system that combines multiple embedding approaches to achieve optimal search performance. The system uses:\n",
    "\n",
    "1. **Matryoshka Embeddings** - For efficient initial retrieval with truncated dimensions\n",
    "2. **ColBERT Embeddings** - For detailed semantic matching with late interaction\n",
    "3. **SPLADE Embeddings** - For sparse lexical retrieval with term expansion\n",
    "4. **Cross-Encoder Reranking** - For final precision ranking\n",
    "\n",
    "The pipeline implements a multi-stage retrieval approach that leverages the strengths of each embedding type while maintaining computational efficiency.\n",
    "\n",
    "## Pipeline Architecture\n",
    "\n",
    "The retrieval pipeline follows this hierarchical approach:\n",
    "\n",
    "1. **Stage 1**: Use 64-dimensional Matryoshka embeddings for initial broad retrieval (500 candidates)\n",
    "2. **Stage 2**: Refine with full 768-dimensional Matryoshka embeddings (100 candidates)\n",
    "3. **Stage 3**: Parallel SPLADE retrieval (100 candidates)\n",
    "4. **Stage 4**: Merge and filter with ColBERT embeddings (50 candidates)\n",
    "5. **Stage 5**: Final reranking with Cross-Encoder (20 final results)\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba7c602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No configuration file specified. Trying the default location\n",
      "WARNING:root:Loading configuration from /Users/chandarl/Documents/GitHub/llm_bootcamp_curriculum/topics/retrieval_funnel/config.yaml if it exists\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from fastembed import LateInteractionTextEmbedding, SparseTextEmbedding\n",
    "from qdrant_client import QdrantClient, models\n",
    "from retrieval_funnel import config\n",
    "from retrieval_funnel.hf_text_utils import get_train_test_lists, tuples_list_to_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0bd04",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "First, we load the subject chunks from the three subjects (Biology, Physics, History) that are used in the matryoshka embeddings notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc9a19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5331 text chunks\n",
      "Subject distribution: Biology=1264, Physics=1295, History=2772\n"
     ]
    }
   ],
   "source": [
    "# Load subject chunks (biology=0, physics=1, history=2)\n",
    "_, test_data = get_train_test_lists(cfg=config)\n",
    "test_dataset = tuples_list_to_dataset(test_data)\n",
    "\n",
    "# Extract text chunks and their labels\n",
    "text_chunks = test_dataset[\"text\"]\n",
    "labels = test_dataset[\"label\"]\n",
    "\n",
    "print(f\"Loaded {len(text_chunks)} text chunks\")\n",
    "print(f\"Subject distribution: Biology={sum(1 for label in labels if label == 0)}, \"\n",
    "      f\"Physics={sum(1 for label in labels if label == 1)}, \"\n",
    "      f\"History={sum(1 for label in labels if label == 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f15247",
   "metadata": {},
   "source": [
    "## 2. Initialize Embedding Models\n",
    "\n",
    "We initialize all embedding models that will be used in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63541d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = 'cuda' if torch.cuda.is_available() else device\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03d7e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Matryoshka Embeddings Model Full 768\n",
    "matryoshka_model = SentenceTransformer(\"tomaarsen/mpnet-base-nli-matryoshka\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "667e6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Matryoshka Embeddings Model Truncated to 64\n",
    "matryoshka_64_model = SentenceTransformer(\"tomaarsen/mpnet-base-nli-matryoshka\", truncate_dim=64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c4181ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. ColBERT Embeddings Model\n",
    "colbert_model = LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "657ffbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. SPLADE Embeddings Model\n",
    "splade_model = SparseTextEmbedding(\"prithivida/Splade_PP_en_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee78957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Cross-Encoder for final reranking\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11aeea3",
   "metadata": {},
   "source": [
    "## 3. Initialize Qdrant Vector Store\n",
    "\n",
    "We create a single collection with multiple vector types to enable efficient hybrid retrieval, following the approach from the [Qdrant reranking tutorial](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b218e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created hybrid collection: hybrid_search\n"
     ]
    }
   ],
   "source": [
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(\"localhost\", port=6333) #QdrantClient(path=\"./qdrant_db\")\n",
    "\n",
    "# Single collection name for all embeddings\n",
    "HYBRID_COLLECTION = \"hybrid_search\"\n",
    "\n",
    "# Create a single collection with multiple vector types\n",
    "try:\n",
    "    qdrant_client.recreate_collection(\n",
    "        collection_name=HYBRID_COLLECTION,\n",
    "        vectors_config={\n",
    "            \"matryoshka_64\": models.VectorParams(\n",
    "                size=64,\n",
    "                distance=models.Distance.COSINE,\n",
    "            ),\n",
    "            \"matryoshka_768\": models.VectorParams(\n",
    "                size=768,\n",
    "                distance=models.Distance.COSINE,\n",
    "            ),\n",
    "            \"colbert\": models.VectorParams(\n",
    "                size=128,\n",
    "                distance=models.Distance.COSINE,\n",
    "                multivector_config=models.MultiVectorConfig(\n",
    "                    comparator=models.MultiVectorComparator.MAX_SIM,\n",
    "                ),\n",
    "                hnsw_config=models.HnswConfigDiff(m=0)  # Disable HNSW for reranking\n",
    "            ),\n",
    "        },\n",
    "        sparse_vectors_config={\n",
    "            \"splade\": models.SparseVectorParams(\n",
    "                modifier=models.Modifier.IDF\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created hybrid collection: {HYBRID_COLLECTION}\")\n",
    "except Exception as e:\n",
    "    print(f\"Collection {HYBRID_COLLECTION} already exists or error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb8110",
   "metadata": {},
   "source": [
    "## 4. Generate and Store Embeddings\n",
    "\n",
    "We generate embeddings for all text chunks using each model and store them in the hybrid collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d87d7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings_in_qdrant():\n",
    "    \"\"\"Generate and store all embeddings in a single hybrid collection\"\"\"\n",
    "    \n",
    "    # Batch size for processing\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in range(0, len(text_chunks), batch_size):\n",
    "        batch_texts = text_chunks[i:i+batch_size]\n",
    "        batch_labels = labels[i:i+batch_size]\n",
    "        \n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(text_chunks) + batch_size - 1)//batch_size}\")\n",
    "        \n",
    "        # 1. Generate Matryoshka embeddings (64 and 768 dimensions)\n",
    "        matryoshka_64_embeddings = matryoshka_64_model.encode(\n",
    "            batch_texts, \n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        matryoshka_768_embeddings = matryoshka_model.encode(\n",
    "            batch_texts, \n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # 2. Generate ColBERT embeddings\n",
    "        colbert_embeddings = list(colbert_model.embed(batch_texts))\n",
    "        \n",
    "        # 3. Generate SPLADE embeddings\n",
    "        splade_embeddings = list(splade_model.embed(batch_texts))\n",
    "        \n",
    "        # Store in single hybrid collection\n",
    "        points = []\n",
    "        \n",
    "        for j, (text, label) in enumerate(zip(batch_texts, batch_labels)):\n",
    "            doc_id = i + j\n",
    "            \n",
    "            # Prepare payload\n",
    "            payload = {\n",
    "                \"id\": doc_id,\n",
    "                \"text\": text,\n",
    "                \"label\": int(label),\n",
    "                \"subject\": [\"Biology\", \"Physics\", \"History\"][int(label)]\n",
    "            }\n",
    "            \n",
    "            # Create point with all vector types\n",
    "            point = models.PointStruct(\n",
    "                id=doc_id,\n",
    "                payload=payload,\n",
    "                vector={\n",
    "                    \"matryoshka_64\": matryoshka_64_embeddings[j].tolist(),\n",
    "                    \"matryoshka_768\": matryoshka_768_embeddings[j].tolist(),\n",
    "                    \"colbert\": colbert_embeddings[j],\n",
    "                    \"splade\": splade_embeddings[j].as_object()\n",
    "                }\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        # Upload to single collection\n",
    "        qdrant_client.upload_points(HYBRID_COLLECTION, points)\n",
    "    \n",
    "    print(\"All embeddings stored successfully in hybrid collection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d96e8f",
   "metadata": {},
   "source": [
    "### Execute the embedding generation and storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "954c9965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/334\n",
      "Processing batch 2/334\n",
      "Processing batch 3/334\n",
      "Processing batch 4/334\n",
      "Processing batch 5/334\n",
      "Processing batch 6/334\n",
      "Processing batch 7/334\n",
      "Processing batch 8/334\n",
      "Processing batch 9/334\n",
      "Processing batch 10/334\n",
      "Processing batch 11/334\n",
      "Processing batch 12/334\n",
      "Processing batch 13/334\n",
      "Processing batch 14/334\n",
      "Processing batch 15/334\n",
      "Processing batch 16/334\n",
      "Processing batch 17/334\n",
      "Processing batch 18/334\n",
      "Processing batch 19/334\n",
      "Processing batch 20/334\n",
      "Processing batch 21/334\n",
      "Processing batch 22/334\n",
      "Processing batch 23/334\n",
      "Processing batch 24/334\n",
      "Processing batch 25/334\n",
      "Processing batch 26/334\n",
      "Processing batch 27/334\n",
      "Processing batch 28/334\n",
      "Processing batch 29/334\n",
      "Processing batch 30/334\n",
      "Processing batch 31/334\n",
      "Processing batch 32/334\n",
      "Processing batch 33/334\n",
      "Processing batch 34/334\n",
      "Processing batch 35/334\n",
      "Processing batch 36/334\n",
      "Processing batch 37/334\n",
      "Processing batch 38/334\n",
      "Processing batch 39/334\n",
      "Processing batch 40/334\n",
      "Processing batch 41/334\n",
      "Processing batch 42/334\n",
      "Processing batch 43/334\n",
      "Processing batch 44/334\n",
      "Processing batch 45/334\n",
      "Processing batch 46/334\n",
      "Processing batch 47/334\n",
      "Processing batch 48/334\n",
      "Processing batch 49/334\n",
      "Processing batch 50/334\n",
      "Processing batch 51/334\n",
      "Processing batch 52/334\n",
      "Processing batch 53/334\n",
      "Processing batch 54/334\n",
      "Processing batch 55/334\n",
      "Processing batch 56/334\n",
      "Processing batch 57/334\n",
      "Processing batch 58/334\n",
      "Processing batch 59/334\n",
      "Processing batch 60/334\n",
      "Processing batch 61/334\n",
      "Processing batch 62/334\n",
      "Processing batch 63/334\n",
      "Processing batch 64/334\n",
      "Processing batch 65/334\n",
      "Processing batch 66/334\n",
      "Processing batch 67/334\n",
      "Processing batch 68/334\n",
      "Processing batch 69/334\n",
      "Processing batch 70/334\n",
      "Processing batch 71/334\n",
      "Processing batch 72/334\n",
      "Processing batch 73/334\n",
      "Processing batch 74/334\n",
      "Processing batch 75/334\n",
      "Processing batch 76/334\n",
      "Processing batch 77/334\n",
      "Processing batch 78/334\n",
      "Processing batch 79/334\n",
      "Processing batch 80/334\n",
      "Processing batch 81/334\n",
      "Processing batch 82/334\n",
      "Processing batch 83/334\n",
      "Processing batch 84/334\n",
      "Processing batch 85/334\n",
      "Processing batch 86/334\n",
      "Processing batch 87/334\n",
      "Processing batch 88/334\n",
      "Processing batch 89/334\n",
      "Processing batch 90/334\n",
      "Processing batch 91/334\n",
      "Processing batch 92/334\n",
      "Processing batch 93/334\n",
      "Processing batch 94/334\n",
      "Processing batch 95/334\n",
      "Processing batch 96/334\n",
      "Processing batch 97/334\n",
      "Processing batch 98/334\n",
      "Processing batch 99/334\n",
      "Processing batch 100/334\n",
      "Processing batch 101/334\n",
      "Processing batch 102/334\n",
      "Processing batch 103/334\n",
      "Processing batch 104/334\n",
      "Processing batch 105/334\n",
      "Processing batch 106/334\n",
      "Processing batch 107/334\n",
      "Processing batch 108/334\n",
      "Processing batch 109/334\n",
      "Processing batch 110/334\n",
      "Processing batch 111/334\n",
      "Processing batch 112/334\n",
      "Processing batch 113/334\n",
      "Processing batch 114/334\n",
      "Processing batch 115/334\n",
      "Processing batch 116/334\n",
      "Processing batch 117/334\n",
      "Processing batch 118/334\n",
      "Processing batch 119/334\n",
      "Processing batch 120/334\n",
      "Processing batch 121/334\n",
      "Processing batch 122/334\n",
      "Processing batch 123/334\n",
      "Processing batch 124/334\n",
      "Processing batch 125/334\n",
      "Processing batch 126/334\n",
      "Processing batch 127/334\n",
      "Processing batch 128/334\n",
      "Processing batch 129/334\n",
      "Processing batch 130/334\n",
      "Processing batch 131/334\n",
      "Processing batch 132/334\n",
      "Processing batch 133/334\n",
      "Processing batch 134/334\n",
      "Processing batch 135/334\n",
      "Processing batch 136/334\n",
      "Processing batch 137/334\n",
      "Processing batch 138/334\n",
      "Processing batch 139/334\n",
      "Processing batch 140/334\n",
      "Processing batch 141/334\n",
      "Processing batch 142/334\n",
      "Processing batch 143/334\n",
      "Processing batch 144/334\n",
      "Processing batch 145/334\n",
      "Processing batch 146/334\n",
      "Processing batch 147/334\n",
      "Processing batch 148/334\n",
      "Processing batch 149/334\n",
      "Processing batch 150/334\n",
      "Processing batch 151/334\n",
      "Processing batch 152/334\n",
      "Processing batch 153/334\n",
      "Processing batch 154/334\n",
      "Processing batch 155/334\n",
      "Processing batch 156/334\n",
      "Processing batch 157/334\n",
      "Processing batch 158/334\n",
      "Processing batch 159/334\n",
      "Processing batch 160/334\n",
      "Processing batch 161/334\n",
      "Processing batch 162/334\n",
      "Processing batch 163/334\n",
      "Processing batch 164/334\n",
      "Processing batch 165/334\n",
      "Processing batch 166/334\n",
      "Processing batch 167/334\n",
      "Processing batch 168/334\n",
      "Processing batch 169/334\n",
      "Processing batch 170/334\n",
      "Processing batch 171/334\n",
      "Processing batch 172/334\n",
      "Processing batch 173/334\n",
      "Processing batch 174/334\n",
      "Processing batch 175/334\n",
      "Processing batch 176/334\n",
      "Processing batch 177/334\n",
      "Processing batch 178/334\n",
      "Processing batch 179/334\n",
      "Processing batch 180/334\n",
      "Processing batch 181/334\n",
      "Processing batch 182/334\n",
      "Processing batch 183/334\n",
      "Processing batch 184/334\n",
      "Processing batch 185/334\n",
      "Processing batch 186/334\n",
      "Processing batch 187/334\n",
      "Processing batch 188/334\n",
      "Processing batch 189/334\n",
      "Processing batch 190/334\n",
      "Processing batch 191/334\n",
      "Processing batch 192/334\n",
      "Processing batch 193/334\n",
      "Processing batch 194/334\n",
      "Processing batch 195/334\n",
      "Processing batch 196/334\n",
      "Processing batch 197/334\n",
      "Processing batch 198/334\n",
      "Processing batch 199/334\n",
      "Processing batch 200/334\n",
      "Processing batch 201/334\n",
      "Processing batch 202/334\n",
      "Processing batch 203/334\n",
      "Processing batch 204/334\n",
      "Processing batch 205/334\n",
      "Processing batch 206/334\n",
      "Processing batch 207/334\n",
      "Processing batch 208/334\n",
      "Processing batch 209/334\n",
      "Processing batch 210/334\n",
      "Processing batch 211/334\n",
      "Processing batch 212/334\n",
      "Processing batch 213/334\n",
      "Processing batch 214/334\n",
      "Processing batch 215/334\n",
      "Processing batch 216/334\n",
      "Processing batch 217/334\n",
      "Processing batch 218/334\n",
      "Processing batch 219/334\n",
      "Processing batch 220/334\n",
      "Processing batch 221/334\n",
      "Processing batch 222/334\n",
      "Processing batch 223/334\n",
      "Processing batch 224/334\n",
      "Processing batch 225/334\n",
      "Processing batch 226/334\n",
      "Processing batch 227/334\n",
      "Processing batch 228/334\n",
      "Processing batch 229/334\n",
      "Processing batch 230/334\n",
      "Processing batch 231/334\n",
      "Processing batch 232/334\n",
      "Processing batch 233/334\n",
      "Processing batch 234/334\n",
      "Processing batch 235/334\n",
      "Processing batch 236/334\n",
      "Processing batch 237/334\n",
      "Processing batch 238/334\n",
      "Processing batch 239/334\n",
      "Processing batch 240/334\n",
      "Processing batch 241/334\n",
      "Processing batch 242/334\n",
      "Processing batch 243/334\n",
      "Processing batch 244/334\n",
      "Processing batch 245/334\n",
      "Processing batch 246/334\n",
      "Processing batch 247/334\n",
      "Processing batch 248/334\n",
      "Processing batch 249/334\n",
      "Processing batch 250/334\n",
      "Processing batch 251/334\n",
      "Processing batch 252/334\n",
      "Processing batch 253/334\n",
      "Processing batch 254/334\n",
      "Processing batch 255/334\n",
      "Processing batch 256/334\n",
      "Processing batch 257/334\n",
      "Processing batch 258/334\n",
      "Processing batch 259/334\n",
      "Processing batch 260/334\n",
      "Processing batch 261/334\n",
      "Processing batch 262/334\n",
      "Processing batch 263/334\n",
      "Processing batch 264/334\n",
      "Processing batch 265/334\n",
      "Processing batch 266/334\n",
      "Processing batch 267/334\n",
      "Processing batch 268/334\n",
      "Processing batch 269/334\n",
      "Processing batch 270/334\n",
      "Processing batch 271/334\n",
      "Processing batch 272/334\n",
      "Processing batch 273/334\n",
      "Processing batch 274/334\n",
      "Processing batch 275/334\n",
      "Processing batch 276/334\n",
      "Processing batch 277/334\n",
      "Processing batch 278/334\n",
      "Processing batch 279/334\n",
      "Processing batch 280/334\n",
      "Processing batch 281/334\n",
      "Processing batch 282/334\n",
      "Processing batch 283/334\n",
      "Processing batch 284/334\n",
      "Processing batch 285/334\n",
      "Processing batch 286/334\n",
      "Processing batch 287/334\n",
      "Processing batch 288/334\n",
      "Processing batch 289/334\n",
      "Processing batch 290/334\n",
      "Processing batch 291/334\n",
      "Processing batch 292/334\n",
      "Processing batch 293/334\n",
      "Processing batch 294/334\n",
      "Processing batch 295/334\n",
      "Processing batch 296/334\n",
      "Processing batch 297/334\n",
      "Processing batch 298/334\n",
      "Processing batch 299/334\n",
      "Processing batch 300/334\n",
      "Processing batch 301/334\n",
      "Processing batch 302/334\n",
      "Processing batch 303/334\n",
      "Processing batch 304/334\n",
      "Processing batch 305/334\n",
      "Processing batch 306/334\n",
      "Processing batch 307/334\n",
      "Processing batch 308/334\n",
      "Processing batch 309/334\n",
      "Processing batch 310/334\n",
      "Processing batch 311/334\n",
      "Processing batch 312/334\n",
      "Processing batch 313/334\n",
      "Processing batch 314/334\n",
      "Processing batch 315/334\n",
      "Processing batch 316/334\n",
      "Processing batch 317/334\n",
      "Processing batch 318/334\n",
      "Processing batch 319/334\n",
      "Processing batch 320/334\n",
      "Processing batch 321/334\n",
      "Processing batch 322/334\n",
      "Processing batch 323/334\n",
      "Processing batch 324/334\n",
      "Processing batch 325/334\n",
      "Processing batch 326/334\n",
      "Processing batch 327/334\n",
      "Processing batch 328/334\n",
      "Processing batch 329/334\n",
      "Processing batch 330/334\n",
      "Processing batch 331/334\n",
      "Processing batch 332/334\n",
      "Processing batch 333/334\n",
      "Processing batch 334/334\n",
      "All embeddings stored successfully in hybrid collection!\n"
     ]
    }
   ],
   "source": [
    "store_embeddings_in_qdrant()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a1c9bf",
   "metadata": {},
   "source": [
    "## 5. Multi-Stage Retrieval Pipeline\n",
    "\n",
    "Now we implement the retrieval pipeline that combines all embedding approaches using the `prefetch` feature that Qdrant provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42d156b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEmbeddingRetrievalPipelineQdrant:\n",
    "    \"\"\"Multi-stage retrieval pipeline combining Matryoshka, ColBERT, SPLADE, and Cross-Encoder using sequential refinement\"\"\"\n",
    "    \n",
    "    def __init__(self, qdrant_client, matryoshka_model, matryoshka_64_model, colbert_model, splade_model, cross_encoder):\n",
    "        self.qdrant_client: QdrantClient = qdrant_client\n",
    "        self.matryoshka_model = matryoshka_model\n",
    "        self.matryoshka_64_model = matryoshka_64_model\n",
    "        self.colbert_model = colbert_model\n",
    "        self.splade_model = splade_model\n",
    "        self.cross_encoder = cross_encoder\n",
    "        \n",
    "        # Single collection name\n",
    "        self.hybrid_collection = HYBRID_COLLECTION\n",
    "\n",
    "    def hybrid_search(self, query: str, \n",
    "                      matryoshka_64_limit: int = 500, \n",
    "                      matryoshka_768_limit: int = 100, \n",
    "                      splade_limit: int = 100, \n",
    "                      colbert_limit: int = 50):\n",
    "        \"\"\"Hybrid search pipeline in Qdrant\n",
    "\n",
    "        Args:\n",
    "            query (str): The query to search for\n",
    "            matryoshka_64_limit (int, optional): The number of candidates to retrieve from Matryoshka 64D. Defaults to 500.\n",
    "            matryoshka_768_limit (int, optional): The number of candidates to retrieve from Matryoshka 768D. Defaults to 100.\n",
    "            splade_limit (int, optional): The number of candidates to retrieve from SPLADE. Defaults to 100.\n",
    "            colbert_limit (int, optional): The number of candidates to retrieve from ColBERT. Defaults to 50.\n",
    "\n",
    "        Returns:\n",
    "            list: The list of candidates\n",
    "        \"\"\"\n",
    "        matryoshka_64_vectors = self.matryoshka_64_model.encode([query], convert_to_numpy=True)[0]\n",
    "        matryoshka_768_vectors = self.matryoshka_model.encode([query], convert_to_numpy=True)[0]\n",
    "        colbert_vectors = list(self.colbert_model.query_embed(query))[0]\n",
    "        splade_vectors = list(self.splade_model.embed([query]))[0]\n",
    "\n",
    "        prefetch_matryoshka_64 = models.Prefetch(\n",
    "            query=matryoshka_64_vectors.tolist(),\n",
    "            using=\"matryoshka_64\",\n",
    "            limit=matryoshka_64_limit,\n",
    "        )\n",
    "        prefetch_matryoshka_768 = models.Prefetch(\n",
    "            query=matryoshka_768_vectors.tolist(),\n",
    "            prefetch=prefetch_matryoshka_64,\n",
    "            using=\"matryoshka_768\",\n",
    "            limit=matryoshka_768_limit,\n",
    "        )\n",
    "        prefetch_splade = models.Prefetch(\n",
    "            query=models.SparseVector(**splade_vectors.as_object()),\n",
    "            using=\"splade\",\n",
    "            limit=splade_limit,\n",
    "        )\n",
    "        prefetch_merged = [prefetch_matryoshka_768, prefetch_splade]\n",
    "\n",
    "        response = self.qdrant_client.query_points(\n",
    "            collection_name=self.hybrid_collection,\n",
    "            prefetch=prefetch_merged,\n",
    "            query=colbert_vectors,\n",
    "            using=\"colbert\",\n",
    "            with_payload=True,\n",
    "            limit=colbert_limit,\n",
    "        )\n",
    "        print(f\"Retrieved {len(response.points)} candidates from the HYBRID search pipeline\")\n",
    "        return response.points          \n",
    "    \n",
    "    def cross_encoder_reranking(self, query: str, candidates, cross_encoder_limit: int = 20):\n",
    "        \"\"\"Reranking using Cross-Encoder\"\"\"\n",
    "        print(f\"Final reranking to {cross_encoder_limit} results with Cross-Encoder...\")\n",
    "        \n",
    "        # Prepare query-document pairs for cross-encoder\n",
    "        query_doc_pairs = []\n",
    "        for result in candidates:\n",
    "            query_doc_pairs.append([query, result.payload[\"text\"]])\n",
    "        \n",
    "        # Get cross-encoder scores\n",
    "        cross_encoder_scores = self.cross_encoder.predict(query_doc_pairs)\n",
    "        \n",
    "        # Combine scores with results\n",
    "        scored_results = []\n",
    "        for i, result in enumerate(candidates):\n",
    "            scored_results.append({\n",
    "                \"id\": result.id,\n",
    "                \"payload\": result.payload,\n",
    "                \"score\": float(cross_encoder_scores[i]),\n",
    "                \"distance\": result.score\n",
    "            })\n",
    "        \n",
    "        # Sort by cross-encoder score (higher is better)\n",
    "        scored_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        \n",
    "        # Return top results\n",
    "        final_results = scored_results[:cross_encoder_limit]\n",
    "        \n",
    "        print(f\"Cross-Encoder reranking complete: {len(final_results)} results\")\n",
    "        return final_results\n",
    "    \n",
    "    def retrieve(self, query: str, use_cross_encoder: bool = True):\n",
    "        \"\"\"Execute the complete multi-stage retrieval pipeline\"\"\"\n",
    "        print(f\"\\n=== Starting Multi-Stage Retrieval for Query: '{query}' ===\\n\")\n",
    "\n",
    "        colbert_results = self.hybrid_search(query)\n",
    "        \n",
    "        if use_cross_encoder:\n",
    "            # Final reranking with Cross-Encoder\n",
    "            final_results = self.cross_encoder_reranking(query, colbert_results, cross_encoder_limit=20)\n",
    "        else:\n",
    "            # Use ColBERT results directly\n",
    "            final_results = [\n",
    "                {\n",
    "                    \"id\": result.id,\n",
    "                    \"payload\": result.payload,\n",
    "                    \"score\": result.score,\n",
    "                    \"distance\": result.score\n",
    "                }\n",
    "                for result in colbert_results[:20]\n",
    "            ]\n",
    "        \n",
    "        print(f\"\\n=== Retrieval Complete: {len(final_results)} final results ===\\n\")\n",
    "        return final_results\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline_qdrant = MultiEmbeddingRetrievalPipelineQdrant(\n",
    "    qdrant_client=qdrant_client,\n",
    "    matryoshka_model=matryoshka_model,\n",
    "    matryoshka_64_model=matryoshka_64_model,\n",
    "    colbert_model=colbert_model,\n",
    "    splade_model=splade_model,\n",
    "    cross_encoder=cross_encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db420e4d",
   "metadata": {},
   "source": [
    "## 6. Testing the Pipeline\n",
    "\n",
    "Let's test the pipeline with various queries to demonstrate its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d180f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline_qdrant(query: str):\n",
    "    \"\"\"Test the multi-embedding retrieval pipeline with a query\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TEST QUERY: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Execute retrieval\n",
    "    results = pipeline_qdrant.retrieve(query)\n",
    "    \n",
    "    # Display top results\n",
    "    print(\"\\nTOP RESULT:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Score: {results[0]['score']:.4f} | Subject: {results[0]['payload']['subject']}\")\n",
    "    print(f\"   Text: {results[0]['payload']['text'][:150]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fd42c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries covering different subjects\n",
    "test_queries = [\n",
    "    \"What is the relationship between force and acceleration?\",\n",
    "    \"How do vaccines work in the human body?\",\n",
    "    \"What were the major events of World War II?\",\n",
    "    \"Explain the concept of velocity in physics\",\n",
    "    \"What is the role of DNA in genetics?\",\n",
    "    \"Who were the key figures in the American Revolution?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a277beb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST QUERY: What is the relationship between force and acceleration?\n",
      "================================================================================\n",
      "\n",
      "=== Starting Multi-Stage Retrieval for Query: 'What is the relationship between force and acceleration?' ===\n",
      "\n",
      "Retrieved 50 candidates from the HYBRID search pipeline\n",
      "Final reranking to 20 results with Cross-Encoder...\n",
      "Cross-Encoder reranking complete: 20 results\n",
      "\n",
      "=== Retrieval Complete: 20 final results ===\n",
      "\n",
      "\n",
      "TOP RESULT:\n",
      "------------------------------------------------------------\n",
      "Score: 0.9102 | Subject: Physics\n",
      "   Text: – force = mass ×acceleration. But now the ‘force’ is something you calculate, as the\n",
      "vector sum of the forces acting on all the separate particles , a...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TEST QUERY: How do vaccines work in the human body?\n",
      "================================================================================\n",
      "\n",
      "=== Starting Multi-Stage Retrieval for Query: 'How do vaccines work in the human body?' ===\n",
      "\n",
      "Retrieved 50 candidates from the HYBRID search pipeline\n",
      "Final reranking to 20 results with Cross-Encoder...\n",
      "Cross-Encoder reranking complete: 20 results\n",
      "\n",
      "=== Retrieval Complete: 20 final results ===\n",
      "\n",
      "\n",
      "TOP RESULT:\n",
      "------------------------------------------------------------\n",
      "Score: -2.0565 | Subject: Biology\n",
      "   Text: Reprinted from MedlinePlus Genetics (https://medlineplus.gov/genetics/) 198\n",
      "9.2   How does gene therapy work?\n",
      "Gene therapy works by altering the genet...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TEST QUERY: What were the major events of World War II?\n",
      "================================================================================\n",
      "\n",
      "=== Starting Multi-Stage Retrieval for Query: 'What were the major events of World War II?' ===\n",
      "\n",
      "Retrieved 50 candidates from the HYBRID search pipeline\n",
      "Final reranking to 20 results with Cross-Encoder...\n",
      "Cross-Encoder reranking complete: 20 results\n",
      "\n",
      "=== Retrieval Complete: 20 final results ===\n",
      "\n",
      "\n",
      "TOP RESULT:\n",
      "------------------------------------------------------------\n",
      "Score: 1.7708 | Subject: History\n",
      "   Text: FIGURE 13.2 Timeline: The Causes and Consequences of World War II. (credit “Mar 1933”: modification of work\n",
      "“Concentration camp dachau aerial view” by...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TEST QUERY: Explain the concept of velocity in physics\n",
      "================================================================================\n",
      "\n",
      "=== Starting Multi-Stage Retrieval for Query: 'Explain the concept of velocity in physics' ===\n",
      "\n",
      "Retrieved 50 candidates from the HYBRID search pipeline\n",
      "Final reranking to 20 results with Cross-Encoder...\n",
      "Cross-Encoder reranking complete: 20 results\n",
      "\n",
      "=== Retrieval Complete: 20 final results ===\n",
      "\n",
      "\n",
      "TOP RESULT:\n",
      "------------------------------------------------------------\n",
      "Score: -0.2244 | Subject: Physics\n",
      "   Text: 358 Week 6: V ector T orque and Angular Momentum\n",
      "Homework for Week 6\n",
      "Problem 1.\n",
      "Physics Concepts: Make this week’s physics concepts summary as you wor...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TEST QUERY: What is the role of DNA in genetics?\n",
      "================================================================================\n",
      "\n",
      "=== Starting Multi-Stage Retrieval for Query: 'What is the role of DNA in genetics?' ===\n",
      "\n",
      "Retrieved 50 candidates from the HYBRID search pipeline\n",
      "Final reranking to 20 results with Cross-Encoder...\n",
      "Cross-Encoder reranking complete: 20 results\n",
      "\n",
      "=== Retrieval Complete: 20 final results ===\n",
      "\n",
      "\n",
      "TOP RESULT:\n",
      "------------------------------------------------------------\n",
      "Score: -1.0406 | Subject: Biology\n",
      "   Text: Reprinted from MedlinePlus Genetics (https://medlineplus.gov/genetics/) 198\n",
      "9.2   How does gene therapy work?\n",
      "Gene therapy works by altering the genet...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TEST QUERY: Who were the key figures in the American Revolution?\n",
      "================================================================================\n",
      "\n",
      "=== Starting Multi-Stage Retrieval for Query: 'Who were the key figures in the American Revolution?' ===\n",
      "\n",
      "Retrieved 50 candidates from the HYBRID search pipeline\n",
      "Final reranking to 20 results with Cross-Encoder...\n",
      "Cross-Encoder reranking complete: 20 results\n",
      "\n",
      "=== Retrieval Complete: 20 final results ===\n",
      "\n",
      "\n",
      "TOP RESULT:\n",
      "------------------------------------------------------------\n",
      "Score: -1.2089 | Subject: History\n",
      "   Text: • English Settlements Figure 6.7\n",
      "• Dutch and French Settlements Figure 6.9\n",
      "• The Seven Years’ War Figure 6.10\n",
      "• Key Battles Figure 6.11\n",
      "• Allies in Eu...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the test\n",
    "for query in test_queries:\n",
    "    test_pipeline_qdrant(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a553d",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis\n",
    "\n",
    "Let's analyze the performance characteristics of each stage in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93492676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pipeline Performance Analysis ===\n",
      "\n",
      "Retrieved 50 candidates from the HYBRID search pipeline\n",
      "Final reranking to 20 results with Cross-Encoder...\n",
      "Cross-Encoder reranking complete: 20 results\n",
      "Stage 1 (Hybrid Search): 0.085s - 50 candidates\n",
      "Stage 2 (Cross-Encoder): 0.163s - 20 final results\n",
      "\n",
      "Total Pipeline Time: 0.248s\n",
      "\n",
      "Final Results Subject Distribution:\n",
      "  Physics: 20 results\n"
     ]
    }
   ],
   "source": [
    "def analyze_pipeline_performance_qdrant():\n",
    "    \"\"\"Analyze the performance characteristics of the pipeline stages\"\"\"\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    test_query = \"What is the relationship between force and acceleration?\"\n",
    "    \n",
    "    print(\"=== Pipeline Performance Analysis ===\\n\")\n",
    "    \n",
    "    # Stage 1 timing\n",
    "    start_time = time.time()\n",
    "    stage1_results = pipeline_qdrant.hybrid_search(test_query)\n",
    "    stage1_time = time.time() - start_time\n",
    "    \n",
    "    # Stage 5 timing\n",
    "    start_time = time.time()\n",
    "    final_results = pipeline_qdrant.cross_encoder_reranking(test_query, stage1_results)\n",
    "    stage2_time = time.time() - start_time\n",
    "    \n",
    "    # Print performance summary\n",
    "    print(f\"Stage 1 (Hybrid Search): {stage1_time:.3f}s - {len(stage1_results)} candidates\")\n",
    "    print(f\"Stage 2 (Cross-Encoder): {stage2_time:.3f}s - {len(final_results)} final results\")\n",
    "    \n",
    "    total_time = stage1_time + stage2_time\n",
    "    print(f\"\\nTotal Pipeline Time: {total_time:.3f}s\")\n",
    "    \n",
    "    # Subject distribution analysis\n",
    "    subject_counts = {}\n",
    "    for result in final_results:\n",
    "        subject = result['payload']['subject']\n",
    "        subject_counts[subject] = subject_counts.get(subject, 0) + 1\n",
    "    \n",
    "    print(\"\\nFinal Results Subject Distribution:\")\n",
    "    for subject, count in subject_counts.items():\n",
    "        print(f\"  {subject}: {count} results\")\n",
    "\n",
    "# Run performance analysis\n",
    "analyze_pipeline_performance_qdrant()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
