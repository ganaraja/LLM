{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5dbbf3f",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://supportvectors.ai/logo-poster-transparent.png\" width=400px style=\"opacity:0.8\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "286126da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!-- Many of the styles here are inspired by: \n",
       "    https://towardsdatascience.com/10-practical-tips-you-need-to-know-to-personalize-jupyter-notebook-fbd202777e20 \n",
       "\n",
       "\n",
       "    On the author's local machine, these exist in the custom.css file. However, in order to keep uniform look and feel, \n",
       "    and at the request of participants, I have added it to this common import-file here.\n",
       "\n",
       "    -->\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Lora:400,700|Montserrat:300\" rel=\"stylesheet\">\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Crimson+Pro&family=Literata&display=swap\" rel=\"stylesheet\">\n",
       "<style>\n",
       "\n",
       "\n",
       "#ipython_notebook::before{\n",
       " content:\"Neural Architectures\";\n",
       "        color: white;\n",
       "        font-weight: bold;\n",
       "        text-transform: uppercase;\n",
       "        font-family: 'Lora',serif;\n",
       "        font-size:16pt;\n",
       "        margin-bottom:15px;\n",
       "        margin-top:15px;\n",
       "\n",
       "}\n",
       "body > #header {\n",
       "    #background: #D15555;\n",
       "    background: linear-gradient(to bottom, indianred 0%, #fff 100%);\n",
       "    opacity: 0.8;\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       ".navbar-default .navbar-nav > li > a, #kernel_indicator {\n",
       "    color: white;\n",
       "    transition: all 0.25s;\n",
       "    font-size:10pt;\n",
       "    font-family: sans-serif;\n",
       "    font-weight:normal;\n",
       "}\n",
       ".navbar-default {\n",
       "    padding-left:100px;\n",
       "    background: none;\n",
       "    border: none;\n",
       "}\n",
       "\n",
       "\n",
       "body > menubar-container {\n",
       "    background-color: wheat;\n",
       "}\n",
       "#ipython_notebook img{                                                                                        \n",
       "    display:block; \n",
       "\n",
       "    background: url(\"https://www.supportvectors.com/wp-content/uploads/2016/03/logo-poster-smaller.png\") no-repeat;\n",
       "    background-size: contain;\n",
       "\n",
       "    padding-left: 600px;\n",
       "    padding-right: 100px;\n",
       "\n",
       "    -moz-box-sizing: border-box;\n",
       "    box-sizing: border-box;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "body {\n",
       " #font-family:  'Literata', serif;\n",
       "    font-family:'Lora', san-serif;\n",
       "    text-align: justify;\n",
       "    font-weight: 400;\n",
       "    font-size: 12pt;\n",
       "}\n",
       "\n",
       "iframe{\n",
       "    width:100%;\n",
       "    min-height:600px;\n",
       "}\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "# font-family: 'Montserrat', sans-serif;\n",
       " font-family:'Lora', serif;\n",
       " font-weight: 200;\n",
       " text-transform: uppercase;\n",
       " color: #EC7063 ;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    color: #000080;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       "#notebook_name {\n",
       "    font-weight: 600;\n",
       "    font-size:20pt;\n",
       "    text-variant:uppercase;\n",
       "    color: wheat; \n",
       "    margin-right:20px;\n",
       "    margin-left:-500px;\n",
       "}\n",
       "#notebook_name:hover {\n",
       "background-color: salmon;\n",
       "}\n",
       "\n",
       "\n",
       ".dataframe { /* dataframe atau table */\n",
       "    background: white;\n",
       "    box-shadow: 0px 1px 2px #bbb;\n",
       "}\n",
       ".dataframe thead th, .dataframe tbody td {\n",
       "    text-align: center;\n",
       "    padding: 1em;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       ".output {\n",
       "    align-items: center;\n",
       "}\n",
       "\n",
       "div.cell {\n",
       "    transition: all 0.25s;\n",
       "    border: none;\n",
       "    position: relative;\n",
       "    top: 0;\n",
       "}\n",
       "div.cell.selected, div.cell.selected.jupyter-soft-selected {\n",
       "    border: none;\n",
       "    background: transparent;\n",
       "    box-shadow: 0 6px 18px #aaa;\n",
       "    z-index: 10;\n",
       "    top: -10px;\n",
       "}\n",
       ".CodeMirror pre, .CodeMirror-dialog, .CodeMirror-dialog .CodeMirror-search-field, .terminal-app .terminal {\n",
       "    font-family: 'Hack' , serif; \n",
       "    font-weight: 500;\n",
       "    font-size: 14pt;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "</style>    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<div style=\"color:#aaa;font-size:8pt\">\n",
       "<hr/>\n",
       "&copy; SupportVectors. All rights reserved. <blockquote>This notebook is the intellectual property of SupportVectors, and part of its training material. \n",
       "Only the participants in SupportVectors workshops are allowed to study the notebooks for educational purposes currently, but is prohibited from copying or using it for any other purposes without written permission.\n",
       "\n",
       "<b> These notebooks are chapters and sections from Asif Qamar's textbook that he is writing on Data Science. So we request you to not circulate the material to others.</b>\n",
       " </blockquote>\n",
       " <hr/>\n",
       "</div>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c6c82f",
   "metadata": {},
   "source": [
    "# SPLADE Embeddings Walkthrough\n",
    "\n",
    "## Introduction\n",
    "\n",
    "SPLADE (Sparse Lexical and Expansion Model) is a model designed to improve information retrieval by combining the strengths of sparse and dense vector representations. It leverages the power of pretrained language models to enhance sparse vector embeddings, allowing for more efficient and accurate search results.\n",
    "\n",
    "SPLADE embeddings are built on the idea of merging sparse and dense retrieval methods. Sparse vectors, like those used in traditional TF-IDF or BM25 models, are efficient and interpretable but suffer from vocabulary mismatch issues. Dense vectors, on the other hand, capture semantic meaning but require extensive data for fine-tuning and are computationally expensive.\n",
    "\n",
    "SPLADE aims to bridge this gap by using a pretrained language model to identify and expand relevant terms, creating a sparse vector that retains the efficiency of traditional methods while incorporating semantic understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b9e29",
   "metadata": {},
   "source": [
    "\n",
    "## From Input Sentence to Sparse Embedding\n",
    "\n",
    "- **Transformer encoding**  \n",
    "  SPLADE passes the input sentence through a transformer (e.g., BERT). Instead of only producing a pooled embedding, it uses the hidden states of each token.\n",
    "\n",
    "- **Vocabulary projection**  \n",
    "  Each hidden state is projected into a vector the size of the whole vocabulary.  \n",
    "  → For every token, the model predicts scores for *all vocabulary words*, not just the input tokens.\n",
    "\n",
    "- **Sparse activation via regularization**  \n",
    "  To avoid dense vectors, SPLADE applies:\n",
    "  - ReLU (to enforce non-negativity)  \n",
    "  - Log-saturation  \n",
    "  - L1 regularization (to enforce sparsity)  \n",
    "\n",
    "  Most dimensions are pruned to zero, leaving only a **small subset of activated vocabulary terms**.  \n",
    "  This explains why the sparse embedding has *more tokens than the input sentence*: the model expands into semantically related words.\n",
    "\n",
    "**Example**:  \n",
    "Input: `\"jaguar speed\"`  \n",
    "Expansion: `\"animal\"`, `\"leopard\"`, `\"fast\"`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c86b66b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Indexing with SPLADE\n",
    "\n",
    "- Each document is processed into a sparse vocabulary vector.  \n",
    "- Non-zero terms are stored in a standard **inverted index**.  \n",
    "- The weight of each activated word acts like a TF-IDF score or BM-25 score.\n",
    "\n",
    "Result: Indexing is efficient and compatible with traditional IR systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92628274",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Search with SPLADE\n",
    "\n",
    "- Queries are processed the same way as documents.  \n",
    "- Retrieval = **sparse dot product** between query and document vectors.  \n",
    "- Matches occur when:\n",
    "  - **Exact words overlap**  \n",
    "  - **Expanded terms overlap** (semantic matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac71c4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Why It Works\n",
    "\n",
    "- **Expansion effect**: Queries and documents get expanded into related words, reducing vocabulary mismatch.  \n",
    "- **Sparse structure**: Interpretable, efficient, and indexable in existing search engines.  \n",
    "- **Hybrid efficiency**: Combines BM25-like sparse retrieval with semantic richness of transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb9c19",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "SPLADE converts a sentence into a **sparse, vocabulary-sized vector** by expanding tokens into semantically related terms.  \n",
    "- **Indexing**: Documents are stored as sparse term-weight vectors.  \n",
    "- **Search**: Queries are expanded the same way, and matching is done via dot product.  \n",
    "- **Benefit**: Captures both lexical overlap *and* semantic similarity within a sparse, efficient framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bac812",
   "metadata": {},
   "source": [
    "## BM42 vs. SPLADE: Key Differences\n",
    "\n",
    "### BM42\n",
    "- **What it is**: A refinement over BM25, where instead of using raw **term frequency (TF)** in the chunk/document, you replace that with a **transformer-derived importance score** for each token.  \n",
    "- **Effect**:  \n",
    "  - Still a **lexical keyword match** → if a query term isn’t present in the document, there is no hit.  \n",
    "  - But among the matching terms, the scoring is much smarter, since the transformer assigns higher weight to “important” words in context and lower weight to stopwords or less relevant tokens.  \n",
    "- **So**: BM42 = *keyword search with improved weighting* (context-aware TF).\n",
    "\n",
    "\n",
    "### SPLADE\n",
    "- **What it is**: Uses a transformer to generate a **sparse vocabulary-sized embedding** by projecting contextualized hidden states onto the entire vocabulary.  \n",
    "- **Effect**:  \n",
    "  - Not restricted to exact query tokens.  \n",
    "  - Documents and queries both expand into semantically related terms.  \n",
    "  - Matches can happen on **synonyms or related words**, not just identical words.  \n",
    "- **So**: SPLADE = *semantic search in sparse space*, since overlap may occur through expanded equivalents.\n",
    "\n",
    "\n",
    "### Key Difference\n",
    "- **BM42**: Purely keyword-based → retrieval depends on **exact term overlap**, but scoring is transformer-aware.  \n",
    "- **SPLADE**: Expands into a much larger vocabulary → retrieval allows **semantic overlap** (e.g., query *“car”* matches doc with *“automobile”*).  \n",
    "\n",
    "\n",
    "Summary:  \n",
    "- **BM42** stays lexical, just with smarter importance weighting.  \n",
    "- **SPLADE** pushes into semantic territory by **expanding the representation space itself**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
