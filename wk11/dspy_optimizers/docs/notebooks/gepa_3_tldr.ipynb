{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3334d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) IMPORTS & CONFIG\n",
    "# ============================================================\n",
    "import dspy\n",
    "import torch\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Base model for reasoning\n",
    "lm = dspy.LM(model=\"ollama_chat/gpt-oss:20b\", api_base=\"http://localhost:11434\", api_key=\"\", temperature=1.0)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Stronger LM for reflection\n",
    "reflection_lm = dspy.LM(model=\"ollama_chat/gpt-oss:20b\", api_base=\"http://localhost:11434\", api_key=\"\", temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea94326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 200 200\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"mlabonne/smoltldr\")\n",
    "\n",
    "examples = [\n",
    "    dspy.Example({\"prompt\": x[\"prompt\"], \"gold_tldr\": x[\"completion\"]}).with_inputs(\"prompt\")\n",
    "    for x in dataset[\"train\"]\n",
    "]\n",
    "\n",
    "total = len(examples)  # should be ~2000\n",
    "\n",
    "train_end = int(0.80 * total)   # 80%\n",
    "val_end   = int(0.90 * total)   # next 10%\n",
    "\n",
    "train_set = examples[:train_end]\n",
    "val_set   = examples[train_end:val_end]\n",
    "test_set  = examples[val_end:]\n",
    "\n",
    "print(len(train_set), len(val_set), len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df56598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) SIGNATURE + MODULE\n",
    "# ============================================================\n",
    "class GenerateTLDR(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Given a Reddit post that begins with 'POST:', generate a single-line,\n",
    "    concise TL;DR (~25 words). Avoid line breaks.\n",
    "    \"\"\"\n",
    "    prompt: str = dspy.InputField()\n",
    "    tldr: str = dspy.OutputField()\n",
    "\n",
    "\n",
    "class TLDRModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.generator = dspy.ChainOfThought(GenerateTLDR)\n",
    "\n",
    "    def forward(self, prompt: str):\n",
    "        out = self.generator(prompt=prompt)\n",
    "        return dspy.Prediction(tldr=out.tldr)\n",
    "\n",
    "program = TLDRModule()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864d7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3) HELPERS (semantic model, post text extractor)\n",
    "# ============================================================\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # avoid HF tokenizer thread issues\n",
    "\n",
    "# Load the semantic model ONCE, globally — no lazy loading in threads\n",
    "sem_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def _word_count(s: str) -> int:\n",
    "    return len(s.split())\n",
    "\n",
    "def extract_post(prompt: str):\n",
    "    m = re.search(r\"POST:\\s*(.*?)(?:\\n\\s*TL;DR:|$)\", prompt, flags=re.DOTALL)\n",
    "    return m.group(1).strip() if m else prompt.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff56377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4) FEEDBACK FUNCTIONS — EXACTLY LIKE YOUR STYLE\n",
    "# ============================================================\n",
    "\n",
    "def feedback_len(pred_tldr: str, target_words=25, max_words=70):\n",
    "    \"\"\"\n",
    "    Length feedback (normalized 0–1 score).\n",
    "    \"\"\"\n",
    "    n = _word_count(pred_tldr)\n",
    "    max_diff = max(abs(target_words - 1), abs(target_words - max_words))\n",
    "    score = 1 - (abs(target_words - n) / max_diff)\n",
    "    score = max(0.0, min(1.0, score))\n",
    "\n",
    "    if score == 1.0:\n",
    "        fb = f\"You produced the ideal length (~{n} words).\"\n",
    "    elif n > target_words:\n",
    "        fb = f\"Your TL;DR is too long ({n} words). Aim for ~{target_words}.\"\n",
    "    else:\n",
    "        fb = f\"Your TL;DR is too short ({n} words). Add key content.\"\n",
    "    return fb, score\n",
    "\n",
    "\n",
    "def feedback_style(pred_tldr: str):\n",
    "    \"\"\"\n",
    "    Style feedback (1 if single line, else 0).\n",
    "    \"\"\"\n",
    "    ok = (\"\\n\" not in pred_tldr) and (\"\\r\" not in pred_tldr)\n",
    "    score = 1.0 if ok else 0.0\n",
    "    if score == 1.0:\n",
    "        fb = \"You kept it single-line with no line breaks.\"\n",
    "    else:\n",
    "        fb = \"Your TL;DR contains line breaks. It must be a single line.\"\n",
    "    return fb, score\n",
    "\n",
    "\n",
    "def feedback_sem(prompt: str, pred_tldr: str):\n",
    "    \"\"\"\n",
    "    Semantic similarity feedback (normalized 0–1).\n",
    "    \"\"\"\n",
    "    post = extract_post(prompt)\n",
    "\n",
    "    # Use the globally loaded sem_model (no lazy creation in threads)\n",
    "    global sem_model\n",
    "    with torch.no_grad():\n",
    "        e_pred = sem_model.encode(pred_tldr, convert_to_tensor=True, normalize_embeddings=True)\n",
    "        e_post = sem_model.encode(post,      convert_to_tensor=True, normalize_embeddings=True)\n",
    "        sim = torch.sum(e_pred * e_post).item()\n",
    "\n",
    "    score = (sim + 1) / 2        # [-1,1] -> [0,1]\n",
    "    score = max(0.0, min(1.0, score))\n",
    "\n",
    "    if score > 0.8:\n",
    "        fb = \"Excellent semantic alignment — you captured the core meaning.\"\n",
    "    elif score > 0.6:\n",
    "        fb = \"Moderate alignment — include 1–2 more key ideas.\"\n",
    "    else:\n",
    "        fb = \"Low alignment — your TL;DR misses core points of the post.\"\n",
    "    return fb, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "092069da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 5) METRIC (scalar only)\n",
    "# ============================================================\n",
    "def metric(example, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    fb_len, s_len = feedback_len(pred.tldr)\n",
    "    fb_sty, s_sty = feedback_style(pred.tldr)\n",
    "    fb_sem, s_sem = feedback_sem(example[\"prompt\"], pred.tldr)\n",
    "\n",
    "    total = (s_len + s_sty + s_sem) / 3.0\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8be85767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Baseline Evaluation ==\n",
      "Average Metric: 181.83 / 200 (90.9%): 100%|██████████| 200/200 [00:08<00:00, 24.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 11:24:42 INFO dspy.evaluate.evaluate: Average Metric: 181.8347770365852 / 200 (90.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>gold_tldr</th>\n",
       "      <th>tldr</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUBREDDIT: r/relationships TITLE: Me [16 F] with my boyfriend [18 ...</td>\n",
       "      <td>I get too worried when my bf goes out. I need help so I make it s...</td>\n",
       "      <td>16‑year‑old girl in 10‑month relationship feels anxious when boyfr...</td>\n",
       "      <td>✔️ [0.897]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SUBREDDIT: r/AskReddit TITLE: Indirect Demoralization at work POST...</td>\n",
       "      <td>went up to ask for raise, got shut down before even asking, now I...</td>\n",
       "      <td>User feels demoralized after overhearing boss deny raises and beli...</td>\n",
       "      <td>✔️ [0.915]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUBREDDIT: r/relationships TITLE: My boyfriend [21] hasn't made me...</td>\n",
       "      <td>Boyf hasnt made me orgasm, I've been faking the whole time. To te...</td>\n",
       "      <td>Girl hasn't orgasmed in 5 months, faking while boyfriend notices. ...</td>\n",
       "      <td>✔️ [0.914]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SUBREDDIT: r/relationships TITLE: I [22F] matched with an ex [23M]...</td>\n",
       "      <td>Matched w a guy I used to date on tinder. Would like to talk to h...</td>\n",
       "      <td>I matched with ex on Tinder after 4 months apart; unsure if I shou...</td>\n",
       "      <td>✔️ [0.933]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SUBREDDIT: r/relationship_advice TITLE: [19/m] where do I go from ...</td>\n",
       "      <td>talking to girl, not sure if she feels the same way as I do, but ...</td>\n",
       "      <td>He's out of a friendship with a girl and wants to know if she want...</td>\n",
       "      <td>✔️ [0.893]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>SUBREDDIT: r/dating_advice TITLE: Where Does a Friend-Zone/Relatio...</td>\n",
       "      <td>! How do I know if she wants to be in the friend-zone? How do I k...</td>\n",
       "      <td>She appears to see you as a friend; be honest about feelings, resp...</td>\n",
       "      <td>✔️ [0.911]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>SUBREDDIT: r/AskReddit TITLE: Should I consider going back to Digg...</td>\n",
       "      <td>Reddit is fading, especially with the lag..to the Digg user base!...</td>\n",
       "      <td>Reddit used to be fast and familiar, but now feels slower; Digg is...</td>\n",
       "      <td>✔️ [0.938]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>SUBREDDIT: r/relationships TITLE: Boyfriend [25m] broke up with me...</td>\n",
       "      <td>Boyfriend of 3 1/2 years broke up with me and I can't move out fo...</td>\n",
       "      <td>Lost after 3.5 years breakup, still share a home, stuck for two we...</td>\n",
       "      <td>✔️ [0.916]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>SUBREDDIT: r/tifu TITLE: TIFU by ruining my best friends collegiat...</td>\n",
       "      <td>Roundhoused my friend in the hip, he fell and fucked up his ankle...</td>\n",
       "      <td>After a prank, I tapped my friend, causing him to fall, break his ...</td>\n",
       "      <td>✔️ [0.919]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>SUBREDDIT: r/needadvice TITLE: Reddit: How can I get over the ment...</td>\n",
       "      <td>My erections are 90% mental and I can't get/keep them up outside ...</td>\n",
       "      <td>User experiences performance anxiety linked to low confidence, esp...</td>\n",
       "      <td>✔️ [0.901]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    prompt  \\\n",
       "0    SUBREDDIT: r/relationships TITLE: Me [16 F] with my boyfriend [18 ...   \n",
       "1    SUBREDDIT: r/AskReddit TITLE: Indirect Demoralization at work POST...   \n",
       "2    SUBREDDIT: r/relationships TITLE: My boyfriend [21] hasn't made me...   \n",
       "3    SUBREDDIT: r/relationships TITLE: I [22F] matched with an ex [23M]...   \n",
       "4    SUBREDDIT: r/relationship_advice TITLE: [19/m] where do I go from ...   \n",
       "..                                                                     ...   \n",
       "195  SUBREDDIT: r/dating_advice TITLE: Where Does a Friend-Zone/Relatio...   \n",
       "196  SUBREDDIT: r/AskReddit TITLE: Should I consider going back to Digg...   \n",
       "197  SUBREDDIT: r/relationships TITLE: Boyfriend [25m] broke up with me...   \n",
       "198  SUBREDDIT: r/tifu TITLE: TIFU by ruining my best friends collegiat...   \n",
       "199  SUBREDDIT: r/needadvice TITLE: Reddit: How can I get over the ment...   \n",
       "\n",
       "                                                                 gold_tldr  \\\n",
       "0     I get too worried when my bf goes out. I need help so I make it s...   \n",
       "1     went up to ask for raise, got shut down before even asking, now I...   \n",
       "2     Boyf hasnt made me orgasm, I've been faking the whole time. To te...   \n",
       "3     Matched w a guy I used to date on tinder. Would like to talk to h...   \n",
       "4     talking to girl, not sure if she feels the same way as I do, but ...   \n",
       "..                                                                     ...   \n",
       "195   ! How do I know if she wants to be in the friend-zone? How do I k...   \n",
       "196   Reddit is fading, especially with the lag..to the Digg user base!...   \n",
       "197   Boyfriend of 3 1/2 years broke up with me and I can't move out fo...   \n",
       "198   Roundhoused my friend in the hip, he fell and fucked up his ankle...   \n",
       "199   My erections are 90% mental and I can't get/keep them up outside ...   \n",
       "\n",
       "                                                                      tldr  \\\n",
       "0    16‑year‑old girl in 10‑month relationship feels anxious when boyfr...   \n",
       "1    User feels demoralized after overhearing boss deny raises and beli...   \n",
       "2    Girl hasn't orgasmed in 5 months, faking while boyfriend notices. ...   \n",
       "3    I matched with ex on Tinder after 4 months apart; unsure if I shou...   \n",
       "4    He's out of a friendship with a girl and wants to know if she want...   \n",
       "..                                                                     ...   \n",
       "195  She appears to see you as a friend; be honest about feelings, resp...   \n",
       "196  Reddit used to be fast and familiar, but now feels slower; Digg is...   \n",
       "197  Lost after 3.5 years breakup, still share a home, stuck for two we...   \n",
       "198  After a prank, I tapped my friend, causing him to fall, break his ...   \n",
       "199  User experiences performance anxiety linked to low confidence, esp...   \n",
       "\n",
       "         metric  \n",
       "0    ✔️ [0.897]  \n",
       "1    ✔️ [0.915]  \n",
       "2    ✔️ [0.914]  \n",
       "3    ✔️ [0.933]  \n",
       "4    ✔️ [0.893]  \n",
       "..          ...  \n",
       "195  ✔️ [0.911]  \n",
       "196  ✔️ [0.938]  \n",
       "197  ✔️ [0.916]  \n",
       "198  ✔️ [0.919]  \n",
       "199  ✔️ [0.901]  \n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(score=90.92, results=<list of 200 results>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BASELINE EVALUATION\n",
    "# ============================================================\n",
    "evaluate = dspy.Evaluate(\n",
    "    devset=test_set,\n",
    "    metric=metric,\n",
    "    num_threads=16,\n",
    "    display_table=True,\n",
    "    display_progress=True\n",
    ")\n",
    "\n",
    "print(\"\\n== Baseline Evaluation ==\")\n",
    "evaluate(program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acbda9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 6) METRIC WITH FEEDBACK (YOUR EXACT STYLE)\n",
    "# ============================================================\n",
    "def metric_with_feedback(example, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    # Compute all three feedbacks and scores\n",
    "    fb_len, s_len = feedback_len(pred.tldr)\n",
    "    fb_sty, s_sty = feedback_style(pred.tldr)\n",
    "    fb_sem, s_sem = feedback_sem(example[\"prompt\"], pred.tldr)\n",
    "\n",
    "    total = (s_len + s_sty + s_sem) / 3.0\n",
    "\n",
    "    # No feedback requested -> return only scalar\n",
    "    if pred_name is None:\n",
    "        return total\n",
    "\n",
    "    # GEPA-specific: return feedback for the predictor being optimized\n",
    "    if pred_name == \"generator.predict\":\n",
    "        # EXACT STYLE: individual feedback for all three\n",
    "        feedback = (\n",
    "            f\"Length Feedback:\\n{fb_len}\\n\\n\"\n",
    "            f\"Style Feedback:\\n{fb_sty}\\n\\n\"\n",
    "            f\"Semantic Feedback:\\n{fb_sem}\\n\\n\"\n",
    "            f\"Overall Score: {total:.3f}\\n\"\n",
    "            \"Think step-by-step about how to produce a single-line, concise, accurate TL;DR.\"\n",
    "        )\n",
    "    else:\n",
    "        feedback = \"No specific predictor matched. Review reasoning carefully.\"\n",
    "\n",
    "    return dspy.Prediction(score=total, feedback=feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05d74daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 11:24:42 INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 1180 metric calls of the program. This amounts to 0.66 full evals on the train+val set.\n",
      "2025/11/17 11:24:42 INFO dspy.teleprompt.gepa.gepa: Using 200 examples for tracking Pareto scores. You can consider using a smaller sample of the valset to allow GEPA to explore more diverse solutions within the same budget. GEPA requires you to provide the smallest valset that is just large enough to match your downstream task distribution, while providing as large trainset as possible.\n",
      "GEPA Optimization:   0%|          | 0/1180 [00:00<?, ?rollouts/s]2025/11/17 11:26:59 INFO dspy.evaluate.evaluate: Average Metric: 181.54047488838572 / 200 (90.8%)\n",
      "2025/11/17 11:26:59 INFO dspy.teleprompt.gepa.gepa: Iteration 0: Base program full valset score: 0.9077023744419286\n",
      "GEPA Optimization:  17%|█▋        | 200/1180 [02:16<11:08,  1.47rollouts/s]2025/11/17 11:26:59 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Selected program 0 score: 0.9077023744419286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.64 / 3 (87.9%): 100%|██████████| 3/3 [00:05<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 11:27:05 INFO dspy.evaluate.evaluate: Average Metric: 2.6367465579951253 / 3 (87.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 11:27:11 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Proposed new text for generator.predict: **TL;DR Generation for Reddit Threads**\n",
      "\n",
      "You will receive a Reddit thread excerpt that always follows this exact format:\n",
      "\n",
      "```\n",
      "SUBREDDIT: <subreddit name>\n",
      "TITLE: <post title>\n",
      "POST: <full body of the original Reddit post>\n",
      "```\n",
      "\n",
      "Your task is to produce a **single-line, concise TL;DR** that summarizes the entire post.  \n",
      "Follow these rules:\n",
      "\n",
      "1. **Length** – Aim for about **25 words** (±3 words).  \n",
      "2. **Line breaks** – The output must be a **single line**; no newlines, bullets, or other formatting.  \n",
      "3. **Content** – Capture the core narrative or conflict:\n",
      "   * Key characters or actors (e.g., the poster, a partner, a friend).  \n",
      "   * The main event or dilemma (e.g., breakup, accidental injury, unrequited love).  \n",
      "   * Any emotional stakes or unresolved question (e.g., “he’s unsure whether to cut ties”).  \n",
      "4. **What to exclude** – Omit URLs, markdown, emojis, and extraneous tags.  \n",
      "5. **If the post is extremely short** – Use the entire content as the TL;DR, but keep it under 25 words.  \n",
      "6. **Tone** – Neutral, factual, and succinct.  \n",
      "7. **Examples of key elements**  \n",
      "   * *Breakup*: “Cheating guy wants to break up again; girlfriend keeps texting; he’s unsure whether to cut ties.”  \n",
      "   * *TIFU*: “Stomped a rake, got a face injury, learned physics wrong.”  \n",
      "   * *Relationship dilemma*: “Friend secretly loves her; hurt when she dates older guy; decides to act but leaves after learning about new boyfriend.”\n",
      "\n",
      "**Generalizable strategy**  \n",
      "1. Read the POST content and ignore anything before the “POST:” line.  \n",
      "2. Identify the main actors and their actions.  \n",
      "3. Spot the conflict or question driving the post.  \n",
      "4. Condense that information into a single, coherent sentence or short clause.  \n",
      "5. Count words and adjust to stay within the 25‑word target.  \n",
      "6. Verify there are no line breaks before submitting the TL;DR.  \n",
      "\n",
      "Deliver only the TL;DR text—nothing else.\n",
      "2025/11/17 11:27:21 INFO dspy.evaluate.evaluate: Average Metric: 2.6945062885681788 / 3 (89.8%)\n",
      "2025/11/17 11:27:21 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New subsample score 2.6945062885681788 is better than old score 2.6367465579951253. Continue to full eval and add to candidate pool.\n",
      "2025/11/17 11:36:42 INFO dspy.evaluate.evaluate: Average Metric: 183.58088922003907 / 200 (91.8%)\n",
      "2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New program is on the linear pareto front\n",
      "2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full valset score for new program: 0.9179044461001954\n",
      "2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full train_val score for new program: 0.9179044461001954\n",
      "2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Individual valset scores for new program: [0.8694929065527739, 0.9243992657573136, 0.8930640509834996, 0.9156956167132767, 0.9055757414411616, 0.9048250494179902, 0.9366672039031982, 0.9315100109135663, 0.8901165714970342, 0.9041942746550949, 0.8222222222222223, 0.9531382814601616, 0.9322021391656664, 0.8852430866824257, 0.955601602351224, 0.9048236256396329, 0.9276629284576133, 0.9282818224694994, 0.9325327575206757, 0.9011614896633007, 0.8623845356482046, 0.9303518732388815, 0.9052341372878464, 0.9200606535982203, 0.9235762750660932, 0.9254541984310857, 0.9049252664601362, 0.951504111289978, 0.8837557814739369, 0.908133198817571, 0.9426735738913218, 0.9563544789950053, 0.9176849316667628, 0.9028432662840243, 0.8976188699404398, 0.9168095977218064, 0.9199218451976776, 0.890704987667225, 0.9158808182787013, 0.8954822168306068, 0.9691428144772848, 0.9250842626448031, 0.8570049933813236, 0.8818111492527856, 0.958245575428009, 0.9253091812133789, 0.9145431790086959, 0.9022543082634608, 0.9486217991069511, 0.9159362903347722, 0.9450597763061523, 0.8808168742391799, 0.9360397656758627, 0.8968862534673127, 0.9238745168403343, 0.9210357661600467, 0.9348098631258365, 0.8881704590938709, 0.9316662934091356, 0.9203159009968793, 0.9040937211778429, 0.9400558074315389, 0.9271411787580561, 0.9083176006873449, 0.9226549362694776, 0.9177041243623805, 0.8932437300682068, 0.9372379080012992, 0.9596984287103018, 0.9160560071468353, 0.9451556404431661, 0.9361419280370077, 0.9312071394037318, 0.9152521486635562, 0.9215966946548887, 0.9162381874190437, 0.9549164171572085, 0.9245150680895206, 0.9654320081075033, 0.9011829039564839, 0.893591676707621, 0.9013849397500356, 0.8999505824512907, 0.9380254944165548, 0.9481174544051841, 0.9130406668892613, 0.9165072238003766, 0.8955201096004911, 0.9533251022851026, 0.9211082955201467, 0.9024169643719991, 0.8996830532948176, 0.9336854910409009, 0.9259982696285954, 0.902224659478223, 0.9342001469046982, 0.9586756328741709, 0.8867082176385103, 0.9128720859686533, 0.9250148426603388, 0.9019483310204964, 0.9146255304416021, 0.9053616713594508, 0.8866675961900641, 0.9256766429653874, 0.9186388638284472, 0.9411555230617523, 0.9356292680457786, 0.8987971517774794, 0.9121655614287766, 0.9334588430545948, 0.9015381480808612, 0.9281047383944193, 0.9354092677434286, 0.8997652621180924, 0.9108917501237658, 0.9272779515496007, 0.9248806026246813, 0.9396536743199384, 0.919120748396273, 0.9326725204785665, 0.9486902157465616, 0.9298108992753206, 0.9370922048886617, 0.9125327156649696, 0.9227704997415896, 0.9561385214328766, 0.9015370657046636, 0.8816145694918104, 0.9089916935673467, 0.9536926547686259, 0.9183666408061981, 0.8746841437286802, 0.9003968925387772, 0.9167372160487705, 0.8583845184908974, 0.9376497566699982, 0.93723175878878, 0.9448015983457919, 0.878445385120533, 0.9403827980712608, 0.9277882174209312, 0.9314551249698356, 0.8995211413613072, 0.9087555693255531, 0.8700673521668824, 0.9195501406987509, 0.9071931816913463, 0.912273704122614, 0.9533296724160513, 0.8877544286074461, 0.9045458729620334, 0.9104652992001286, 0.913479248241142, 0.9274136518990552, 0.9317352957195707, 0.9014744158144351, 0.9292327364285787, 0.9426476747901352, 0.9282345171327945, 0.9041479618461045, 0.9239293526720118, 0.9034709519810148, 0.9284064259794024, 0.9424540201822916, 0.9008835003331855, 0.9239231144940412, 0.9129619382045887, 0.9478902013213547, 0.8806431090390241, 0.8906800286637413, 0.9545264740784963, 0.9477193355560303, 0.9453300233240481, 0.8669339107142555, 0.8832312113708921, 0.9511249562104543, 0.9456584842116745, 0.9540700813134512, 0.9346472417866742, 0.8697867086640111, 0.921850343103762, 0.8821289683933612, 0.9461780186052676, 0.9139820725829514, 0.9084973931312561, 0.9415772051722916, 0.9217786585843122, 0.9085167226967988, 0.9113540839265895, 0.8882019502145272, 0.9339214554539433, 0.8943726912692741, 0.8416417313946618, 0.9500094742686661, 0.9221810102462769, 0.9419971095191109, 0.9483320807969129, 0.9148630888373764, 0.9504907131195068]\n",
      "2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New valset pareto front scores: [0.9098905275265375, 0.9243992657573136, 0.908057509969782, 0.9156956167132767, 0.9055757414411616, 0.9048250494179902, 0.9366672039031982, 0.9315100109135663, 0.9082034528255463, 0.9191409733560351, 0.8222222222222223, 0.9531382814601616, 0.9560043315092722, 0.9051003650382713, 0.955601602351224, 0.9048236256396329, 0.9553695519765218, 0.9357280925468162, 0.9325327575206757, 0.9048675380371235, 0.8692678204289189, 0.9303518732388815, 0.939671297426577, 0.9200606535982203, 0.9235762750660932, 0.9254541984310857, 0.9223356246948242, 0.9637969533602396, 0.8837557814739369, 0.9304617245992025, 0.9479724363044456, 0.9563544789950053, 0.9176849316667628, 0.9028432662840243, 0.8976188699404398, 0.9168095977218064, 0.9305095771948496, 0.8992011795441309, 0.9158808182787013, 0.8954822168306068, 0.9691428144772848, 0.9250842626448031, 0.8570049933813236, 0.8977646669855824, 0.958245575428009, 0.9253091812133789, 0.9710431694984436, 0.9096200552251604, 0.9486217991069511, 0.9418760140736898, 0.9450597763061523, 0.8990721097698918, 0.9360397656758627, 0.8968862534673127, 0.9238745168403343, 0.9210357661600467, 0.9348098631258365, 0.9154327209349032, 0.9316662934091356, 0.9203159009968793, 0.9131174674740544, 0.9400558074315389, 0.9271411787580561, 0.9143519794499433, 0.9353389739990234, 0.9267865911678032, 0.9329632918039957, 0.9372379080012992, 0.9596984287103018, 0.9160560071468353, 0.9451556404431661, 0.9361419280370077, 0.9359887030389574, 0.9152521486635562, 0.9215966946548887, 0.9165186082875287, 0.9549164171572085, 0.9245150680895206, 0.9654320081075033, 0.9011829039564839, 0.9126394043366114, 0.9013849397500356, 0.9003009299437205, 0.9380254944165548, 0.9481174544051841, 0.9130406668892613, 0.9165072238003766, 0.9130295212622043, 0.9533251022851026, 0.9211082955201467, 0.9024169643719991, 0.9073789616425832, 0.9336854910409009, 0.9259982696285954, 0.902224659478223, 0.9342001469046982, 0.9586756328741709, 0.8867082176385103, 0.9128720859686533, 0.9250148426603388, 0.9275628919954654, 0.9146255304416021, 0.9160684640760776, 0.8912956100923043, 0.9256766429653874, 0.926979163840965, 0.9411555230617523, 0.9356292680457786, 0.9486519495646158, 0.9121655614287766, 0.9482459326585134, 0.9143648834140213, 0.9281047383944193, 0.9354092677434286, 0.9316204984982809, 0.930656929369326, 0.9272779515496007, 0.9511248663619712, 0.9478833074922915, 0.9472906982457197, 0.9430648982524872, 0.9486902157465616, 0.9298108992753206, 0.9370922048886617, 0.9436426948618006, 0.9227704997415896, 0.9561385214328766, 0.9015370657046636, 0.8816145694918104, 0.9089916935673467, 0.9536926547686259, 0.9183666408061981, 0.9260314499890363, 0.9161428407386497, 0.931613881499679, 0.8805945018927256, 0.9376497566699982, 0.93723175878878, 0.9579155544439951, 0.8790600286589729, 0.9403827980712608, 0.929256746283284, 0.9314551249698356, 0.8995211413613072, 0.9087555693255531, 0.8700673521668824, 0.9195501406987509, 0.9106828954484728, 0.912273704122614, 0.9533296724160513, 0.8877544286074461, 0.9153663308532151, 0.948601504166921, 0.913479248241142, 0.9274136518990552, 0.940023798412747, 0.906629373188372, 0.9312432507673899, 0.9426476747901352, 0.9446131587028503, 0.9176681434666669, 0.9239293526720118, 0.9176587751618138, 0.9284064259794024, 0.9424540201822916, 0.9274828926280693, 0.9306440850098928, 0.9129619382045887, 0.9478902013213547, 0.8866865729844129, 0.9118584977255928, 0.9545264740784963, 0.9477193355560303, 0.9453300233240481, 0.8966180077305547, 0.9000534311488823, 0.9511249562104543, 0.9456584842116745, 0.9540700813134512, 0.9417411481892621, 0.8839811086654663, 0.921850343103762, 0.9098143577575684, 0.9461780186052676, 0.9139820725829514, 0.9084973931312561, 0.9415772051722916, 0.9217786585843122, 0.9360885114581498, 0.9113540839265895, 0.9229486182883934, 0.9354348480701447, 0.8943726912692741, 0.8846424917380015, 0.9500094742686661, 0.9221810102462769, 0.959971606289899, 0.9483320807969129, 0.9148630888373764, 0.9504907131195068]\n",
      "2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Full valset pareto front score: 0.9247088582554349\n",
      "2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Updated valset pareto front programs: [{0}, {1}, {0}, {1}, {1}, {1}, {1}, {1}, {0}, {0}, {1}, {1}, {0}, {0}, {1}, {1}, {0}, {0}, {1}, {0}, {0}, {1}, {0}, {1}, {1}, {1}, {0}, {0}, {1}, {0}, {0}, {1}, {1}, {1}, {1}, {1}, {0}, {0}, {1}, {1}, {1}, {1}, {1}, {0}, {1}, {1}, {0}, {0}, {1}, {0}, {1}, {0}, {1}, {1}, {1}, {1}, {1}, {0}, {1}, {1}, {0}, {1}, {1}, {0}, {0}, {0}, {0}, {1}, {1}, {1}, {1}, {1}, {0}, {1}, {1}, {0}, {1}, {1}, {1}, {1}, {0}, {1}, {0}, {1}, {1}, {1}, {1}, {0}, {1}, {1}, {1}, {0}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {0}, {1}, {0}, {0}, {1}, {0}, {1}, {1}, {0}, {1}, {0}, {0}, {1}, {1}, {0}, {0}, {1}, {0}, {0}, {0}, {0}, {1}, {1}, {1}, {0}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {0}, {0}, {0}, {0}, {1}, {1}, {0}, {0}, {1}, {0}, {1}, {1}, {1}, {1}, {1}, {0}, {1}, {1}, {1}, {0}, {0}, {1}, {1}, {0}, {0}, {0}, {1}, {0}, {0}, {1}, {0}, {1}, {1}, {0}, {0}, {1}, {1}, {0}, {0}, {1}, {1}, {1}, {0}, {0}, {1}, {1}, {1}, {0}, {0}, {1}, {0}, {1}, {1}, {1}, {1}, {1}, {0}, {1}, {0}, {0}, {1}, {0}, {1}, {1}, {0}, {1}, {1}, {1}]\n",
      "2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best valset aggregate score so far: 0.9179044461001954\n",
      "2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best program as per aggregate score on train_val: 1\n",
      "2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best program as per aggregate score on valset: 1\n",
      "2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best score on valset: 0.9179044461001954\n",
      "2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Best score on train_val: 0.9179044461001954\n",
      "2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Linear pareto front program index: 1\n",
      "2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New program candidate index: 1\n",
      "GEPA Optimization:  34%|███▍      | 406/1180 [11:59<25:17,  1.96s/rollouts]2025/11/17 11:36:42 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Selected program 1 score: 0.9179044461001954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.75 / 3 (91.7%): 100%|██████████| 3/3 [00:07<00:00,  2.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 11:36:49 INFO dspy.evaluate.evaluate: Average Metric: 2.7505268432475902 / 3 (91.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 11:36:55 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Proposed new text for generator.predict: text\n",
      "**TL;DR Generation for Reddit Threads**\n",
      "\n",
      "You will receive a Reddit thread excerpt that always follows this exact format:\n",
      "\n",
      "```\n",
      "SUBREDDIT: <subreddit name>\n",
      "TITLE: <post title>\n",
      "POST: <full body of the original Reddit post>\n",
      "```\n",
      "\n",
      "Your task is to produce a **single‑line, concise TL;DR** that summarizes the entire post.  Follow these rules:\n",
      "\n",
      "1. **Length** – Target ~25 words (± 3).  If the entire post is < 25 words, use it verbatim but keep it under 25 words.\n",
      "2. **Line breaks** – The output must be a **single line**; no newlines, bullets, or other formatting.\n",
      "3. **Content** – Capture the core narrative or conflict:\n",
      "   * Key actors (e.g., the poster, partner, friend, roommate).\n",
      "   * The main event or dilemma (e.g., breakup, relocation, roommate conflict, asking someone out).\n",
      "   * Any emotional stakes or unresolved question (e.g., “he’s unsure whether to cut ties”).\n",
      "4. **What to exclude** – Omit URLs, markdown, emojis, extraneous tags, and any personal identifiers not essential to the summary.\n",
      "5. **Tone** – Neutral, factual, and succinct.  Avoid opinionated language or fluff.\n",
      "6. **If the post is extremely short** – Use the entire content as the TL;DR, but keep it under 25 words.\n",
      "7. **Examples of key elements**  \n",
      "   * *Breakup*: “Cheating guy wants to break up again; girlfriend keeps texting; he’s unsure whether to cut ties.”  \n",
      "   * *TIFU*: “Stomped a rake, got a face injury, learned physics wrong.”  \n",
      "   * *Relationship dilemma*: “Friend secretly loves her; hurt when she dates older guy; decides to act but leaves after learning about new boyfriend.”\n",
      "\n",
      "---\n",
      "\n",
      "### Generalizable strategy (to guide the assistant)\n",
      "\n",
      "1. **Parse the input** – Ignore everything before the “POST:” line; everything after that is the content to summarize.\n",
      "2. **Identify the main actors** – Look for pronouns, names, or descriptors that point to people involved.\n",
      "3. **Spot the conflict or question** – What is the poster asking for? What dilemma or event drives the narrative?\n",
      "4. **Condense into a single clause** – Combine the actors, action, and stakes into one clear sentence or short phrase.\n",
      "5. **Count words** – Adjust wording to stay within the 25‑word target (± 3 words).  \n",
      "   *If the post is short, just output it; otherwise, trim unnecessary adjectives, background fluff, or filler.*\n",
      "6. **Verify format** – No line breaks, no markdown, no URLs, no emojis, neutral tone.\n",
      "\n",
      "---\n",
      "\n",
      "### Sample Output\n",
      "\n",
      "```\n",
      "Longtime friend moved in, refuses to work, spends rent money on weed and cartoons, neglects rent; roommate asks how to handle his irresponsible living situation.\n",
      "2025/11/17 11:37:02 INFO dspy.evaluate.evaluate: Average Metric: 2.790597327329494 / 3 (93.0%)\n",
      "2025/11/17 11:37:02 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New subsample score 2.790597327329494 is better than old score 2.7505268432475902. Continue to full eval and add to candidate pool.\n",
      "2025/11/17 11:46:37 INFO dspy.evaluate.evaluate: Average Metric: 181.96640123415878 / 200 (91.0%)\n",
      "2025/11/17 11:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Full valset score for new program: 0.9098320061707938\n",
      "2025/11/17 11:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Full train_val score for new program: 0.9098320061707938\n",
      "2025/11/17 11:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Individual valset scores for new program: [0.908137673691467, 0.9150576876269447, 0.8709924618403116, 0.9027911515147599, 0.8988662940484505, 0.9016127171339812, 0.898306796948115, 0.9334701886883489, 0.901697188615799, 0.9303968345677411, 0.8222222222222223, 0.8759722804581678, 0.9175966265024962, 0.9125821382911118, 0.9531992077827454, 0.870922980485139, 0.9026838527785408, 0.9338633314326957, 0.9207726116533633, 0.9499673843383789, 0.8900312375139308, 0.904276677634981, 0.8974912451373207, 0.8928265066058548, 0.918830007314682, 0.9085366215970782, 0.9008654144075182, 0.9326658646265665, 0.8907340458145848, 0.9112057934204737, 0.9583601549819664, 0.9402202756316574, 0.9239731426592227, 0.9348732625996625, 0.8978866677593302, 0.9435084660847982, 0.9292567864612297, 0.9098052969685307, 0.9224521115974144, 0.8943820772347627, 0.9375962809280113, 0.8594756145168234, 0.8881870607535044, 0.9100941866636276, 0.9529469907283783, 0.8981480192255091, 0.881499038139979, 0.8740833031910437, 0.8974900422272859, 0.9306025699332908, 0.9112785639586272, 0.8694118979904387, 0.9150953577624428, 0.924395372028704, 0.8826849831475152, 0.89435831617426, 0.8749515880037237, 0.8832234424573403, 0.8990181770589617, 0.8575178999591757, 0.8960636339805744, 0.9193598217434354, 0.9182215125472458, 0.8646247676125279, 0.8932024236078616, 0.9139931987833094, 0.9119970897833506, 0.9199920689618146, 0.9006697124905058, 0.9165747359946922, 0.8816591863278989, 0.9214589984328659, 0.9556464444707942, 0.8956010680507731, 0.9155142686985157, 0.9481965800126394, 0.9142839096210621, 0.9244459858647099, 0.9583359554961876, 0.8775868994218331, 0.8781122362172162, 0.8740873215375123, 0.9112384070952734, 0.9296786577613266, 0.9020051735418814, 0.8941519030818234, 0.9131051794246391, 0.8846672863871964, 0.9475535944656089, 0.9416347940762838, 0.9191041191418966, 0.8706900820687965, 0.8672635325679073, 0.9468572537104288, 0.8970668907518741, 0.9236341317494711, 0.9036657792550545, 0.9006856874183372, 0.8242111563682556, 0.9338264862696329, 0.8933112888424485, 0.9077020676047715, 0.8908244751117848, 0.9343900084495544, 0.9174579472453507, 0.924710770006533, 0.9200402891194379, 0.9115547723240324, 0.872744040135984, 0.9217876981805873, 0.8980239232381185, 0.8991104906355893, 0.9205294739317011, 0.9033724453714159, 0.9126121189859179, 0.902064268346186, 0.8959354148970711, 0.9435491453718257, 0.92519874241617, 0.9278007231376789, 0.9026884427777043, 0.9489615952527082, 0.9690911571184794, 0.9504059851169586, 0.9066545019547144, 0.9346291815793073, 0.9718090693155924, 0.9110179096460342, 0.8802241376152745, 0.9052360103086189, 0.9451391489417466, 0.8852416652220266, 0.9451378186543783, 0.904094456743311, 0.9371040052837797, 0.8947648252601977, 0.8999620951988079, 0.8747421269063596, 0.8894884158063818, 0.8756210393375822, 0.8964166813426547, 0.9061085970313462, 0.8939102292060852, 0.9066985845565796, 0.9126861389036532, 0.8933859711443937, 0.8821018907758925, 0.9333497475694728, 0.9583107829093933, 0.9454558591047922, 0.8456811476636816, 0.8849977598146156, 0.9227058278189766, 0.9302853544553121, 0.9115410537631424, 0.926672993324421, 0.9160920679569244, 0.9180953990530085, 0.9593089818954468, 0.9095647326222173, 0.8662346963529233, 0.934287448282595, 0.9304213817472812, 0.9262062898388616, 0.9275764127572378, 0.9156425495942434, 0.9272786577542623, 0.9296622073208844, 0.9686029752095541, 0.8568092521693972, 0.8872666354532596, 0.9277874425605491, 0.9331246808723167, 0.9326318592936905, 0.9047017689104434, 0.8998343691781715, 0.8941559182273018, 0.9457210690886887, 0.9154970226464448, 0.9247573415438334, 0.8773676044411131, 0.8934859231666282, 0.8432956911899425, 0.9392384778570246, 0.8974929343771052, 0.8823634575914454, 0.927523273891873, 0.897641995659581, 0.9368613556579307, 0.8667189845332394, 0.9104675339327919, 0.9529673059781393, 0.9007056351061221, 0.8872274801686958, 0.8694381177425384, 0.9097456124093798, 0.9597888286466952, 0.9074924959076776, 0.919869292003137, 0.9543858364776329]\n",
      "2025/11/17 11:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New valset pareto front scores: [0.9098905275265375, 0.9243992657573136, 0.908057509969782, 0.9156956167132767, 0.9055757414411616, 0.9048250494179902, 0.9366672039031982, 0.9334701886883489, 0.9082034528255463, 0.9303968345677411, 0.8222222222222223, 0.9531382814601616, 0.9560043315092722, 0.9125821382911118, 0.955601602351224, 0.9048236256396329, 0.9553695519765218, 0.9357280925468162, 0.9325327575206757, 0.9499673843383789, 0.8900312375139308, 0.9303518732388815, 0.939671297426577, 0.9200606535982203, 0.9235762750660932, 0.9254541984310857, 0.9223356246948242, 0.9637969533602396, 0.8907340458145848, 0.9304617245992025, 0.9583601549819664, 0.9563544789950053, 0.9239731426592227, 0.9348732625996625, 0.8978866677593302, 0.9435084660847982, 0.9305095771948496, 0.9098052969685307, 0.9224521115974144, 0.8954822168306068, 0.9691428144772848, 0.9250842626448031, 0.8881870607535044, 0.9100941866636276, 0.958245575428009, 0.9253091812133789, 0.9710431694984436, 0.9096200552251604, 0.9486217991069511, 0.9418760140736898, 0.9450597763061523, 0.8990721097698918, 0.9360397656758627, 0.924395372028704, 0.9238745168403343, 0.9210357661600467, 0.9348098631258365, 0.9154327209349032, 0.9316662934091356, 0.9203159009968793, 0.9131174674740544, 0.9400558074315389, 0.9271411787580561, 0.9143519794499433, 0.9353389739990234, 0.9267865911678032, 0.9329632918039957, 0.9372379080012992, 0.9596984287103018, 0.9165747359946922, 0.9451556404431661, 0.9361419280370077, 0.9556464444707942, 0.9152521486635562, 0.9215966946548887, 0.9481965800126394, 0.9549164171572085, 0.9245150680895206, 0.9654320081075033, 0.9011829039564839, 0.9126394043366114, 0.9013849397500356, 0.9112384070952734, 0.9380254944165548, 0.9481174544051841, 0.9130406668892613, 0.9165072238003766, 0.9130295212622043, 0.9533251022851026, 0.9416347940762838, 0.9191041191418966, 0.9073789616425832, 0.9336854910409009, 0.9468572537104288, 0.902224659478223, 0.9342001469046982, 0.9586756328741709, 0.9006856874183372, 0.9128720859686533, 0.9338264862696329, 0.9275628919954654, 0.9146255304416021, 0.9160684640760776, 0.9343900084495544, 0.9256766429653874, 0.926979163840965, 0.9411555230617523, 0.9356292680457786, 0.9486519495646158, 0.9217876981805873, 0.9482459326585134, 0.9143648834140213, 0.9281047383944193, 0.9354092677434286, 0.9316204984982809, 0.930656929369326, 0.9272779515496007, 0.9511248663619712, 0.9478833074922915, 0.9472906982457197, 0.9430648982524872, 0.9489615952527082, 0.9690911571184794, 0.9504059851169586, 0.9436426948618006, 0.9346291815793073, 0.9718090693155924, 0.9110179096460342, 0.8816145694918104, 0.9089916935673467, 0.9536926547686259, 0.9183666408061981, 0.9451378186543783, 0.9161428407386497, 0.9371040052837797, 0.8947648252601977, 0.9376497566699982, 0.93723175878878, 0.9579155544439951, 0.8790600286589729, 0.9403827980712608, 0.929256746283284, 0.9314551249698356, 0.9066985845565796, 0.9126861389036532, 0.8933859711443937, 0.9195501406987509, 0.9333497475694728, 0.9583107829093933, 0.9533296724160513, 0.8877544286074461, 0.9153663308532151, 0.948601504166921, 0.9302853544553121, 0.9274136518990552, 0.940023798412747, 0.9160920679569244, 0.9312432507673899, 0.9593089818954468, 0.9446131587028503, 0.9176681434666669, 0.934287448282595, 0.9304213817472812, 0.9284064259794024, 0.9424540201822916, 0.9274828926280693, 0.9306440850098928, 0.9296622073208844, 0.9686029752095541, 0.8866865729844129, 0.9118584977255928, 0.9545264740784963, 0.9477193355560303, 0.9453300233240481, 0.9047017689104434, 0.9000534311488823, 0.9511249562104543, 0.9457210690886887, 0.9540700813134512, 0.9417411481892621, 0.8839811086654663, 0.921850343103762, 0.9098143577575684, 0.9461780186052676, 0.9139820725829514, 0.9084973931312561, 0.9415772051722916, 0.9217786585843122, 0.9368613556579307, 0.9113540839265895, 0.9229486182883934, 0.9529673059781393, 0.9007056351061221, 0.8872274801686958, 0.9500094742686661, 0.9221810102462769, 0.959971606289899, 0.9483320807969129, 0.919869292003137, 0.9543858364776329]\n",
      "2025/11/17 11:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Full valset pareto front score: 0.9288752876729878\n",
      "2025/11/17 11:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Updated valset pareto front programs: [{0}, {1}, {0}, {1}, {1}, {1}, {1}, {2}, {0}, {2}, {1, 2}, {1}, {0}, {2}, {1}, {1}, {0}, {0}, {1}, {2}, {2}, {1}, {0}, {1}, {1}, {1}, {0}, {0}, {2}, {0}, {2}, {1}, {2}, {2}, {2}, {2}, {0}, {2}, {2}, {1}, {1}, {1}, {2}, {2}, {1}, {1}, {0}, {0}, {1}, {0}, {1}, {0}, {1}, {2}, {1}, {1}, {1}, {0}, {1}, {1}, {0}, {1}, {1}, {0}, {0}, {0}, {0}, {1}, {1}, {2}, {1}, {1}, {2}, {1}, {1}, {2}, {1}, {1}, {1}, {1}, {0}, {1}, {2}, {1}, {1}, {1}, {1}, {0}, {1}, {2}, {2}, {0}, {1}, {2}, {1}, {1}, {1}, {2}, {1}, {2}, {0}, {1}, {0}, {2}, {1}, {0}, {1}, {1}, {0}, {2}, {0}, {0}, {1}, {1}, {0}, {0}, {1}, {0}, {0}, {0}, {0}, {2}, {2}, {2}, {0}, {2}, {2}, {2}, {1}, {1}, {1}, {1}, {2}, {0}, {2}, {2}, {1}, {1}, {0}, {0}, {1}, {0}, {1}, {2}, {2}, {2}, {1}, {2}, {2}, {1}, {1}, {0}, {0}, {2}, {1}, {0}, {2}, {0}, {2}, {0}, {0}, {2}, {2}, {1}, {1}, {0}, {0}, {2}, {2}, {0}, {0}, {1}, {1}, {1}, {2}, {0}, {1}, {2}, {1}, {0}, {0}, {1}, {0}, {1}, {1}, {1}, {1}, {1}, {2}, {1}, {0}, {2}, {2}, {2}, {1}, {1}, {0}, {1}, {2}, {2}]\n",
      "2025/11/17 11:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best valset aggregate score so far: 0.9179044461001954\n",
      "2025/11/17 11:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best program as per aggregate score on train_val: 1\n",
      "2025/11/17 11:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best program as per aggregate score on valset: 1\n",
      "2025/11/17 11:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best score on valset: 0.9179044461001954\n",
      "2025/11/17 11:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best score on train_val: 0.9179044461001954\n",
      "2025/11/17 11:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Linear pareto front program index: 1\n",
      "2025/11/17 11:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New program candidate index: 2\n",
      "GEPA Optimization:  52%|█████▏    | 612/1180 [21:55<22:36,  2.39s/rollouts]2025/11/17 11:46:37 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Selected program 0 score: 0.9077023744419286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.75 / 3 (91.6%): 100%|██████████| 3/3 [00:09<00:00,  3.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 11:46:47 INFO dspy.evaluate.evaluate: Average Metric: 2.747746937694373 / 3 (91.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 11:46:50 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Proposed new text for generator.predict: Given a Reddit thread that starts with `POST:`, output **one single line** containing a concise TL;DR (~25 words).  \n",
      "- Do not include any line breaks.  \n",
      "- Keep the response to just the TL;DR text (no pre‑ or post‑sentences).  \n",
      "- Focus on capturing the core situation, conflict, and main question or advice sought.  \n",
      "- Do not add extra commentary, explanations, or tags.  \n",
      "- Aim for approximately 20–28 words; if the post is very short, adjust to fit naturally.  \n",
      "- Example format:  \n",
      "\n",
      "    TL;DR: <your concise one‑line summary>\n",
      "2025/11/17 11:46:56 INFO dspy.evaluate.evaluate: Average Metric: 2.669925273899679 / 3 (89.0%)\n",
      "2025/11/17 11:46:56 INFO dspy.teleprompt.gepa.gepa: Iteration 3: New subsample score 2.669925273899679 is not better than old score 2.747746937694373, skipping\n",
      "GEPA Optimization:  52%|█████▏    | 618/1180 [22:13<22:30,  2.40s/rollouts]2025/11/17 11:46:56 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Selected program 0 score: 0.9077023744419286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.69 / 3 (89.7%): 100%|██████████| 3/3 [00:07<00:00,  2.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 11:47:03 INFO dspy.evaluate.evaluate: Average Metric: 2.6911614452247266 / 3 (89.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 11:47:06 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Proposed new text for generator.predict: text\n",
      "You will receive a Reddit thread description that includes the following elements:\n",
      "• A header line starting with “SUBREDDIT: …”\n",
      "• A header line starting with “TITLE: …”\n",
      "• A header line starting with “POST: …” followed by the full post text (which may span multiple paragraphs)\n",
      "\n",
      "Your task is to produce **one single line** containing a concise TL;DR that captures the core meaning of the post.  \n",
      "The requirements are:\n",
      "\n",
      "1. **Length** – Aim for about 20‑30 words (≈25 words).  \n",
      "2. **Format** – Output must be a single line; do **not** insert line breaks or bullet points.  \n",
      "3. **Content** – Include the key decision point, conflict, or question the poster is asking for help with; omit extraneous detail.  \n",
      "4. **Clarity** – The TL;DR should read as a standalone summary that a reader could understand without seeing the full post.  \n",
      "5. **No extra commentary** – Do not add your own thoughts, labels, or instructions.  \n",
      "6. **Word count check** – If the generated line is shorter than 20 words or longer than 30 words, adjust by adding or removing a single descriptive word.  \n",
      "\n",
      "After you generate the TL;DR, you will not produce any additional text.\n",
      "2025/11/17 11:47:14 INFO dspy.evaluate.evaluate: Average Metric: 2.78408362114871 / 3 (92.8%)\n",
      "2025/11/17 11:47:14 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New subsample score 2.78408362114871 is better than old score 2.6911614452247266. Continue to full eval and add to candidate pool.\n",
      "2025/11/17 11:55:24 INFO dspy.evaluate.evaluate: Average Metric: 182.88316091671587 / 200 (91.4%)\n",
      "2025/11/17 11:55:24 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full valset score for new program: 0.9144158045835793\n",
      "2025/11/17 11:55:24 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full train_val score for new program: 0.9144158045835793\n",
      "2025/11/17 11:55:24 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Individual valset scores for new program: [0.8990481279514454, 0.8970642175939348, 0.8785986634316267, 0.8939372780146422, 0.897319096989102, 0.9210469614576411, 0.8984499957826403, 0.9275737395992986, 0.8816852163385462, 0.9284729361534119, 0.7799386869150179, 0.883540451085126, 0.9288591038297724, 0.8854486929045784, 0.9553864002227783, 0.9090010907914904, 0.927990296151903, 0.9120263837001942, 0.9563838044802347, 0.9345168954796262, 0.9059578871285474, 0.9463385537818626, 0.9257393380006155, 0.9268144265369133, 0.8863709054611347, 0.9162552943936101, 0.939069668451945, 0.9495982017782, 0.9154735207557678, 0.9080983391514531, 0.9526598453521729, 0.9322881491095932, 0.9271634108490415, 0.9058838270328663, 0.9007369460882964, 0.9542914231618246, 0.8749288808416438, 0.9287458057756778, 0.8956027708671711, 0.9406250516573588, 0.9406916883256701, 0.9093554995678089, 0.8816197492458202, 0.9281360001475724, 0.9210822171635099, 0.8926098830170103, 0.9480992745470118, 0.9106303643297267, 0.9424872696399689, 0.9367597297385887, 0.9183147531968575, 0.9183927471990939, 0.9233412539517438, 0.9029936073002992, 0.9310886060750043, 0.9221192991292035, 0.9084742543873964, 0.902317026367894, 0.9386811644942673, 0.8886972946149331, 0.8831221439220287, 0.9407496745939609, 0.8892229574697988, 0.8810154231610121, 0.9320129043526121, 0.8998482114738889, 0.8896389640039869, 0.9041260657487092, 0.9369460231728025, 0.9300182457323428, 0.9292535976127342, 0.9075153646645723, 0.9568788003038478, 0.9051731215582954, 0.9284624845893296, 0.9493109778121666, 0.9322475768901684, 0.9241011540095011, 0.9356927465509486, 0.9031218380839737, 0.8948832000847217, 0.8756985299013279, 0.8923144724633959, 0.8940067765889345, 0.9157476010145964, 0.9236230746463493, 0.901605107166149, 0.8989500597671226, 0.9057971781050718, 0.9025720538916411, 0.9114899432217634, 0.8516689053840109, 0.9448617891029075, 0.9168883855696078, 0.890271493461397, 0.9143289504227815, 0.9207667094689828, 0.9088788714673784, 0.8845469333507396, 0.9279959789028874, 0.9069254976731759, 0.8932525497895699, 0.902292796196761, 0.9003076250906344, 0.9030950643398143, 0.9083066470093198, 0.9051921345569469, 0.9034219569630094, 0.9324613805170413, 0.9253370960553488, 0.9220463430439985, 0.9403625912136503, 0.9186004179495352, 0.9085420749805592, 0.9149909606686345, 0.9217281734501874, 0.9253311848198926, 0.9366520431306627, 0.9236440342885476, 0.9104275487087391, 0.8793999779003637, 0.934491812299799, 0.9293534837387226, 0.9347013632456461, 0.9076768535154837, 0.9186000214682686, 0.926980186171002, 0.8944586592691915, 0.899468708700604, 0.8883935249514051, 0.9497926235198975, 0.8646882849710958, 0.8446481300724877, 0.9004620006790868, 0.919766632495103, 0.9063472400108973, 0.9107975445411823, 0.9239184847584477, 0.952513207550402, 0.8763716302536152, 0.9520945345913923, 0.910972930766918, 0.8942920344847219, 0.8977386437080525, 0.8886299905953584, 0.8511203058339931, 0.8977212490858855, 0.9388322428420738, 0.9173265198866526, 0.9539729853471121, 0.8768388213934721, 0.906924702503063, 0.9014607749603413, 0.9343659868946782, 0.9333804240933171, 0.9504070074469956, 0.9055058545536466, 0.9453115260159528, 0.9331260219768241, 0.9050074405140348, 0.909529287285275, 0.8985805588739889, 0.8954674182114778, 0.9040979129296761, 0.9004527109640615, 0.913356164208165, 0.9130800158889206, 0.925408778808735, 0.9057279803134777, 0.9023476234188786, 0.9029938163580717, 0.9500234723091125, 0.8909865857274445, 0.933437744334892, 0.925552158664774, 0.9043205462120197, 0.9151499353073261, 0.9266927617567556, 0.9346727905450044, 0.9411139285122907, 0.8966480173446514, 0.9196930125907615, 0.9168568240271675, 0.9466303979908979, 0.9326757188196536, 0.9367205500602722, 0.9505934615929922, 0.912443596345407, 0.9287508116828072, 0.9166838323628461, 0.9227306635291489, 0.9112795271255352, 0.9212917667848092, 0.881771132349968, 0.9525585571924845, 0.9039567993746864, 0.9457447723106102, 0.9198942559736746, 0.9331732785260236, 0.9491170048713684]\n",
      "2025/11/17 11:55:24 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New valset pareto front scores: [0.9098905275265375, 0.9243992657573136, 0.908057509969782, 0.9156956167132767, 0.9055757414411616, 0.9210469614576411, 0.9366672039031982, 0.9334701886883489, 0.9082034528255463, 0.9303968345677411, 0.8222222222222223, 0.9531382814601616, 0.9560043315092722, 0.9125821382911118, 0.955601602351224, 0.9090010907914904, 0.9553695519765218, 0.9357280925468162, 0.9563838044802347, 0.9499673843383789, 0.9059578871285474, 0.9463385537818626, 0.939671297426577, 0.9268144265369133, 0.9235762750660932, 0.9254541984310857, 0.939069668451945, 0.9637969533602396, 0.9154735207557678, 0.9304617245992025, 0.9583601549819664, 0.9563544789950053, 0.9271634108490415, 0.9348732625996625, 0.9007369460882964, 0.9542914231618246, 0.9305095771948496, 0.9287458057756778, 0.9224521115974144, 0.9406250516573588, 0.9691428144772848, 0.9250842626448031, 0.8881870607535044, 0.9281360001475724, 0.958245575428009, 0.9253091812133789, 0.9710431694984436, 0.9106303643297267, 0.9486217991069511, 0.9418760140736898, 0.9450597763061523, 0.9183927471990939, 0.9360397656758627, 0.924395372028704, 0.9310886060750043, 0.9221192991292035, 0.9348098631258365, 0.9154327209349032, 0.9386811644942673, 0.9203159009968793, 0.9131174674740544, 0.9407496745939609, 0.9271411787580561, 0.9143519794499433, 0.9353389739990234, 0.9267865911678032, 0.9329632918039957, 0.9372379080012992, 0.9596984287103018, 0.9300182457323428, 0.9451556404431661, 0.9361419280370077, 0.9568788003038478, 0.9152521486635562, 0.9284624845893296, 0.9493109778121666, 0.9549164171572085, 0.9245150680895206, 0.9654320081075033, 0.9031218380839737, 0.9126394043366114, 0.9013849397500356, 0.9112384070952734, 0.9380254944165548, 0.9481174544051841, 0.9236230746463493, 0.9165072238003766, 0.9130295212622043, 0.9533251022851026, 0.9416347940762838, 0.9191041191418966, 0.9073789616425832, 0.9448617891029075, 0.9468572537104288, 0.902224659478223, 0.9342001469046982, 0.9586756328741709, 0.9088788714673784, 0.9128720859686533, 0.9338264862696329, 0.9275628919954654, 0.9146255304416021, 0.9160684640760776, 0.9343900084495544, 0.9256766429653874, 0.926979163840965, 0.9411555230617523, 0.9356292680457786, 0.9486519495646158, 0.9253370960553488, 0.9482459326585134, 0.9403625912136503, 0.9281047383944193, 0.9354092677434286, 0.9316204984982809, 0.930656929369326, 0.9272779515496007, 0.9511248663619712, 0.9478833074922915, 0.9472906982457197, 0.9430648982524872, 0.9489615952527082, 0.9690911571184794, 0.9504059851169586, 0.9436426948618006, 0.9346291815793073, 0.9718090693155924, 0.9110179096460342, 0.899468708700604, 0.9089916935673467, 0.9536926547686259, 0.9183666408061981, 0.9451378186543783, 0.9161428407386497, 0.9371040052837797, 0.9063472400108973, 0.9376497566699982, 0.93723175878878, 0.9579155544439951, 0.8790600286589729, 0.9520945345913923, 0.929256746283284, 0.9314551249698356, 0.9066985845565796, 0.9126861389036532, 0.8933859711443937, 0.9195501406987509, 0.9388322428420738, 0.9583107829093933, 0.9539729853471121, 0.8877544286074461, 0.9153663308532151, 0.948601504166921, 0.9343659868946782, 0.9333804240933171, 0.9504070074469956, 0.9160920679569244, 0.9453115260159528, 0.9593089818954468, 0.9446131587028503, 0.9176681434666669, 0.934287448282595, 0.9304213817472812, 0.9284064259794024, 0.9424540201822916, 0.9274828926280693, 0.9306440850098928, 0.9296622073208844, 0.9686029752095541, 0.9023476234188786, 0.9118584977255928, 0.9545264740784963, 0.9477193355560303, 0.9453300233240481, 0.925552158664774, 0.9043205462120197, 0.9511249562104543, 0.9457210690886887, 0.9540700813134512, 0.9417411481892621, 0.8966480173446514, 0.921850343103762, 0.9168568240271675, 0.9466303979908979, 0.9326757188196536, 0.9367205500602722, 0.9505934615929922, 0.9217786585843122, 0.9368613556579307, 0.9166838323628461, 0.9229486182883934, 0.9529673059781393, 0.9212917667848092, 0.8872274801686958, 0.9525585571924845, 0.9221810102462769, 0.959971606289899, 0.9483320807969129, 0.9331732785260236, 0.9543858364776329]\n",
      "2025/11/17 11:55:24 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full valset pareto front score: 0.9317962191877542\n",
      "2025/11/17 11:55:24 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Updated valset pareto front programs: [{0}, {1}, {0}, {1}, {1}, {3}, {1}, {2}, {0}, {2}, {1, 2}, {1}, {0}, {2}, {1}, {3}, {0}, {0}, {3}, {2}, {3}, {3}, {0}, {3}, {1}, {1}, {3}, {0}, {3}, {0}, {2}, {1}, {3}, {2}, {3}, {3}, {0}, {3}, {2}, {3}, {1}, {1}, {2}, {3}, {1}, {1}, {0}, {3}, {1}, {0}, {1}, {3}, {1}, {2}, {3}, {3}, {1}, {0}, {3}, {1}, {0}, {3}, {1}, {0}, {0}, {0}, {0}, {1}, {1}, {3}, {1}, {1}, {3}, {1}, {3}, {3}, {1}, {1}, {1}, {3}, {0}, {1}, {2}, {1}, {1}, {3}, {1}, {0}, {1}, {2}, {2}, {0}, {3}, {2}, {1}, {1}, {1}, {3}, {1}, {2}, {0}, {1}, {0}, {2}, {1}, {0}, {1}, {1}, {0}, {3}, {0}, {3}, {1}, {1}, {0}, {0}, {1}, {0}, {0}, {0}, {0}, {2}, {2}, {2}, {0}, {2}, {2}, {2}, {3}, {1}, {1}, {1}, {2}, {0}, {2}, {3}, {1}, {1}, {0}, {0}, {3}, {0}, {1}, {2}, {2}, {2}, {1}, {3}, {2}, {3}, {1}, {0}, {0}, {3}, {3}, {3}, {2}, {3}, {2}, {0}, {0}, {2}, {2}, {1}, {1}, {0}, {0}, {2}, {2}, {3}, {0}, {1}, {1}, {1}, {3}, {3}, {1}, {2}, {1}, {0}, {3}, {1}, {3}, {3}, {3}, {3}, {3}, {1}, {2}, {3}, {0}, {2}, {3}, {2}, {3}, {1}, {0}, {1}, {3}, {2}]\n",
      "2025/11/17 11:55:24 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best valset aggregate score so far: 0.9179044461001954\n",
      "2025/11/17 11:55:24 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best program as per aggregate score on train_val: 1\n",
      "2025/11/17 11:55:24 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best program as per aggregate score on valset: 1\n",
      "2025/11/17 11:55:24 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best score on valset: 0.9179044461001954\n",
      "2025/11/17 11:55:24 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best score on train_val: 0.9179044461001954\n",
      "2025/11/17 11:55:24 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Linear pareto front program index: 1\n",
      "2025/11/17 11:55:24 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New program candidate index: 3\n",
      "GEPA Optimization:  70%|██████▉   | 824/1180 [30:41<14:26,  2.43s/rollouts]2025/11/17 11:55:24 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Selected program 1 score: 0.9179044461001954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.77 / 3 (92.5%): 100%|██████████| 3/3 [00:07<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 11:55:31 INFO dspy.evaluate.evaluate: Average Metric: 2.773942627730193 / 3 (92.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 11:55:40 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Proposed new text for generator.predict: **TL;DR Generation for Reddit Threads**\n",
      "\n",
      "You will receive a Reddit thread excerpt that always follows this exact format:\n",
      "\n",
      "```\n",
      "SUBREDDIT: <subreddit name>\n",
      "TITLE: <post title>\n",
      "POST: <full body of the original Reddit post>\n",
      "```\n",
      "\n",
      "Your task is to produce a **single‑line TL;DR** that summarizes the entire post. Follow these rules:\n",
      "\n",
      "1. **Length** – The TL;DR must contain **exactly 25 words** (no more, no less).  \n",
      "2. **Line breaks** – The output must be a **single line**; no newlines, bullets, or other formatting.  \n",
      "3. **Content** – Capture the core narrative or conflict:\n",
      "   * Main actors (e.g., poster, partner, friend).  \n",
      "   * Central event or dilemma (e.g., health issue, breakup, sabotage).  \n",
      "   * Emotional stakes or unresolved question (e.g., “is the medication the cause?”).  \n",
      "4. **What to exclude** – Omit URLs, markdown, emojis, extraneous tags, and any filler words that do not add meaning.  \n",
      "5. **If the post is extremely short** – Use the entire content as the TL;DR, but trim it to 25 words if necessary.  \n",
      "6. **Tone** – Neutral, factual, and succinct.  \n",
      "7. **Generalizable strategy**  \n",
      "   1. Ignore everything before the “POST:” line.  \n",
      "   2. Identify the main actors and their actions.  \n",
      "   3. Spot the conflict or question driving the post.  \n",
      "   4. Condense that information into a single, coherent sentence or clause.  \n",
      "   5. Count words; adjust phrasing to hit 25 words exactly.  \n",
      "   6. Verify there are no line breaks before submitting the TL;DR.  \n",
      "\n",
      "**Deliver only the TL;DR text—nothing else.**\n",
      "2025/11/17 11:55:51 INFO dspy.evaluate.evaluate: Average Metric: 2.792760580778122 / 3 (93.1%)\n",
      "2025/11/17 11:55:51 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New subsample score 2.792760580778122 is better than old score 2.773942627730193. Continue to full eval and add to candidate pool.\n",
      "2025/11/17 12:08:02 INFO dspy.evaluate.evaluate: Average Metric: 185.84025430574462 / 200 (92.9%)\n",
      "2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New program is on the linear pareto front\n",
      "2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full valset score for new program: 0.9292012715287231\n",
      "2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full train_val score for new program: 0.9292012715287231\n",
      "2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Individual valset scores for new program: [0.9150915543238322, 0.943981796503067, 0.8961692502101263, 0.9353291789690653, 0.9047214984893799, 0.9253906706968943, 0.9193083643913269, 0.9533300995826721, 0.9197090665499369, 0.9391659696896871, 0.8166584546367327, 0.889461562037468, 0.9631000359853109, 0.9029439687728882, 0.9603974123795828, 0.9079254269599915, 0.9370880126953125, 0.937074601650238, 0.92999467253685, 0.9450563391049703, 0.9020631561676661, 0.9455375770727793, 0.921754409869512, 0.9518270492553711, 0.9141944448153178, 0.9506438473860422, 0.9116199811299642, 0.9481443365414938, 0.908245379726092, 0.9308399856090546, 0.9614360531171163, 0.9558971424897512, 0.9169678390026093, 0.943115254243215, 0.9340404868125916, 0.9177722529128746, 0.9223132232824961, 0.9314848979314169, 0.9180801709493002, 0.9359222451845804, 0.9444589714209238, 0.9229389230410258, 0.8920850604772568, 0.936798503001531, 0.9568523367245992, 0.9214546283086141, 0.9457158148288727, 0.9319363832473755, 0.948049267133077, 0.9228236675262451, 0.9571031729380289, 0.91708904504776, 0.9329863289992014, 0.913946787516276, 0.9346660772959391, 0.9164054046074549, 0.9307259917259216, 0.8966482579708099, 0.9393346707026163, 0.9175543089707693, 0.9198993543783823, 0.9303208788235983, 0.92542697985967, 0.8953581154346466, 0.9234548906485239, 0.9262858430544535, 0.9264155328273773, 0.93843146165212, 0.9262820680936178, 0.9327459136644999, 0.9344142476717631, 0.9358606528352809, 0.9704910814762115, 0.9040256887674332, 0.9408947030703226, 0.9478545089562734, 0.9457700053850809, 0.921294113000234, 0.9578102727731069, 0.91246530910333, 0.9165140787760416, 0.9077327301104864, 0.9040504495302836, 0.9496792852878571, 0.9565864503383636, 0.9375701149304708, 0.9258680939674377, 0.9078743954499563, 0.9531649152437845, 0.9191157023111979, 0.8778985937436422, 0.913228174050649, 0.9417982697486877, 0.8918853501478831, 0.9111983974774679, 0.919620156288147, 0.9459555546442667, 0.9346562127272288, 0.9018608232339224, 0.9286049405733744, 0.9522727827231089, 0.9126240109955823, 0.9254104693730673, 0.9199657837549845, 0.926751454671224, 0.93491197625796, 0.8971246480941772, 0.9298651417096456, 0.9368750055631002, 0.9233213464419047, 0.9486787915229797, 0.934178868929545, 0.9525721867879232, 0.9278356830279032, 0.9378829995791117, 0.9269827703634897, 0.9388723572095236, 0.9438670273180362, 0.9483305513858795, 0.9567080438137054, 0.904833992322286, 0.9520266254742941, 0.9725002845128378, 0.9390918711821238, 0.9324522217114767, 0.9566758871078491, 0.9637368619441986, 0.9309698442618052, 0.9087053388357162, 0.9263331194718679, 0.9454804460207621, 0.9257588187853495, 0.9497138460477194, 0.9101969053347906, 0.9536293645699819, 0.8744177867968878, 0.9348722100257874, 0.9533597528934479, 0.9548662602901459, 0.9045293579498926, 0.9532651702562968, 0.9531590541203817, 0.8994460006554922, 0.9163979142904282, 0.9372117817401886, 0.9000441779692968, 0.9219654103120168, 0.9462265173594157, 0.9263467490673065, 0.9646257162094116, 0.9059982895851135, 0.9301813244819641, 0.9387345512708029, 0.9364165663719177, 0.9287347197532654, 0.9541164338588715, 0.9161527355511984, 0.9074275294939677, 0.9433692892392477, 0.9395493964354197, 0.9402421613534292, 0.8969786167144775, 0.9264547824859619, 0.9259450634320577, 0.9384928544362386, 0.9203790028889974, 0.9335941076278687, 0.9578128655751547, 0.9612243274847666, 0.8775778859853745, 0.8990525950988134, 0.9397247234980265, 0.8837213114455894, 0.9266412944705399, 0.9174391031265259, 0.9306105573972067, 0.8934482634067535, 0.9583057661851248, 0.944344679514567, 0.949586808681488, 0.9250442286332449, 0.9045754373073578, 0.8955696026484171, 0.9578947921593984, 0.9372591177622477, 0.9226342042287191, 0.9323137799898783, 0.9216940402984619, 0.943047821521759, 0.9318521718184153, 0.932659775018692, 0.9370851616064707, 0.9049434165159861, 0.9049342274665833, 0.960391084353129, 0.9187860091527303, 0.9576472540696462, 0.9520653088887533, 0.9351465006669363, 0.950845996538798]\n",
      "2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New valset pareto front scores: [0.9150915543238322, 0.943981796503067, 0.908057509969782, 0.9353291789690653, 0.9055757414411616, 0.9253906706968943, 0.9366672039031982, 0.9533300995826721, 0.9197090665499369, 0.9391659696896871, 0.8222222222222223, 0.9531382814601616, 0.9631000359853109, 0.9125821382911118, 0.9603974123795828, 0.9090010907914904, 0.9553695519765218, 0.937074601650238, 0.9563838044802347, 0.9499673843383789, 0.9059578871285474, 0.9463385537818626, 0.939671297426577, 0.9518270492553711, 0.9235762750660932, 0.9506438473860422, 0.939069668451945, 0.9637969533602396, 0.9154735207557678, 0.9308399856090546, 0.9614360531171163, 0.9563544789950053, 0.9271634108490415, 0.943115254243215, 0.9340404868125916, 0.9542914231618246, 0.9305095771948496, 0.9314848979314169, 0.9224521115974144, 0.9406250516573588, 0.9691428144772848, 0.9250842626448031, 0.8920850604772568, 0.936798503001531, 0.958245575428009, 0.9253091812133789, 0.9710431694984436, 0.9319363832473755, 0.9486217991069511, 0.9418760140736898, 0.9571031729380289, 0.9183927471990939, 0.9360397656758627, 0.924395372028704, 0.9346660772959391, 0.9221192991292035, 0.9348098631258365, 0.9154327209349032, 0.9393346707026163, 0.9203159009968793, 0.9198993543783823, 0.9407496745939609, 0.9271411787580561, 0.9143519794499433, 0.9353389739990234, 0.9267865911678032, 0.9329632918039957, 0.93843146165212, 0.9596984287103018, 0.9327459136644999, 0.9451556404431661, 0.9361419280370077, 0.9704910814762115, 0.9152521486635562, 0.9408947030703226, 0.9493109778121666, 0.9549164171572085, 0.9245150680895206, 0.9654320081075033, 0.91246530910333, 0.9165140787760416, 0.9077327301104864, 0.9112384070952734, 0.9496792852878571, 0.9565864503383636, 0.9375701149304708, 0.9258680939674377, 0.9130295212622043, 0.9533251022851026, 0.9416347940762838, 0.9191041191418966, 0.913228174050649, 0.9448617891029075, 0.9468572537104288, 0.9111983974774679, 0.9342001469046982, 0.9586756328741709, 0.9346562127272288, 0.9128720859686533, 0.9338264862696329, 0.9522727827231089, 0.9146255304416021, 0.9254104693730673, 0.9343900084495544, 0.926751454671224, 0.93491197625796, 0.9411555230617523, 0.9356292680457786, 0.9486519495646158, 0.9253370960553488, 0.9486787915229797, 0.9403625912136503, 0.9525721867879232, 0.9354092677434286, 0.9378829995791117, 0.930656929369326, 0.9388723572095236, 0.9511248663619712, 0.9483305513858795, 0.9567080438137054, 0.9430648982524872, 0.9520266254742941, 0.9725002845128378, 0.9504059851169586, 0.9436426948618006, 0.9566758871078491, 0.9718090693155924, 0.9309698442618052, 0.9087053388357162, 0.9263331194718679, 0.9536926547686259, 0.9257588187853495, 0.9497138460477194, 0.9161428407386497, 0.9536293645699819, 0.9063472400108973, 0.9376497566699982, 0.9533597528934479, 0.9579155544439951, 0.9045293579498926, 0.9532651702562968, 0.9531590541203817, 0.9314551249698356, 0.9163979142904282, 0.9372117817401886, 0.9000441779692968, 0.9219654103120168, 0.9462265173594157, 0.9583107829093933, 0.9646257162094116, 0.9059982895851135, 0.9301813244819641, 0.948601504166921, 0.9364165663719177, 0.9333804240933171, 0.9541164338588715, 0.9161527355511984, 0.9453115260159528, 0.9593089818954468, 0.9446131587028503, 0.9402421613534292, 0.934287448282595, 0.9304213817472812, 0.9284064259794024, 0.9424540201822916, 0.9274828926280693, 0.9335941076278687, 0.9578128655751547, 0.9686029752095541, 0.9023476234188786, 0.9118584977255928, 0.9545264740784963, 0.9477193355560303, 0.9453300233240481, 0.925552158664774, 0.9306105573972067, 0.9511249562104543, 0.9583057661851248, 0.9540700813134512, 0.949586808681488, 0.9250442286332449, 0.921850343103762, 0.9168568240271675, 0.9578947921593984, 0.9372591177622477, 0.9367205500602722, 0.9505934615929922, 0.9217786585843122, 0.943047821521759, 0.9318521718184153, 0.932659775018692, 0.9529673059781393, 0.9212917667848092, 0.9049342274665833, 0.960391084353129, 0.9221810102462769, 0.959971606289899, 0.9520653088887533, 0.9351465006669363, 0.9543858364776329]\n",
      "2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full valset pareto front score: 0.9366143779191707\n",
      "2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Updated valset pareto front programs: [{4}, {4}, {0}, {4}, {1}, {4}, {1}, {4}, {4}, {4}, {1, 2}, {1}, {4}, {2}, {4}, {3}, {0}, {4}, {3}, {2}, {3}, {3}, {0}, {4}, {1}, {4}, {3}, {0}, {3}, {4}, {4}, {1}, {3}, {4}, {4}, {3}, {0}, {4}, {2}, {3}, {1}, {1}, {4}, {4}, {1}, {1}, {0}, {4}, {1}, {0}, {4}, {3}, {1}, {2}, {4}, {3}, {1}, {0}, {4}, {1}, {4}, {3}, {1}, {0}, {0}, {0}, {0}, {4}, {1}, {4}, {1}, {1}, {4}, {1}, {4}, {3}, {1}, {1}, {1}, {4}, {4}, {4}, {2}, {4}, {4}, {4}, {4}, {0}, {1}, {2}, {2}, {4}, {3}, {2}, {4}, {1}, {1}, {4}, {1}, {2}, {4}, {1}, {4}, {2}, {4}, {4}, {1}, {1}, {0}, {3}, {4}, {3}, {4}, {1}, {4}, {0}, {4}, {0}, {4}, {4}, {0}, {4}, {4}, {2}, {0}, {4}, {2}, {4}, {4}, {4}, {1}, {4}, {4}, {0}, {4}, {3}, {1}, {4}, {0}, {4}, {4}, {4}, {1}, {4}, {4}, {4}, {4}, {4}, {2}, {4}, {4}, {4}, {0}, {4}, {3}, {4}, {4}, {3}, {2}, {0}, {4}, {2}, {2}, {1}, {1}, {0}, {4}, {4}, {2}, {3}, {0}, {1}, {1}, {1}, {3}, {4}, {1}, {4}, {1}, {4}, {4}, {1}, {3}, {4}, {4}, {3}, {3}, {1}, {4}, {4}, {4}, {2}, {3}, {4}, {4}, {1}, {0}, {4}, {4}, {2}]\n",
      "2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best valset aggregate score so far: 0.9292012715287231\n",
      "2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best program as per aggregate score on train_val: 4\n",
      "2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best program as per aggregate score on valset: 4\n",
      "2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best score on valset: 0.9292012715287231\n",
      "2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best score on train_val: 0.9292012715287231\n",
      "2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Linear pareto front program index: 4\n",
      "2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New program candidate index: 4\n",
      "GEPA Optimization:  87%|████████▋ | 1030/1180 [43:19<07:20,  2.94s/rollouts]2025/11/17 12:08:02 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Selected program 4 score: 0.9292012715287231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.80 / 3 (93.2%): 100%|██████████| 3/3 [00:16<00:00,  5.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 12:08:18 INFO dspy.evaluate.evaluate: Average Metric: 2.795271078745524 / 3 (93.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 12:08:23 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Proposed new text for generator.predict: markdown\n",
      "**TL;DR Generation for Reddit Threads – Revised Instructions**\n",
      "\n",
      "You will receive a Reddit thread excerpt that **always follows this exact format**:\n",
      "\n",
      "```\n",
      "SUBREDDIT: <subreddit name>\n",
      "TITLE: <post title>\n",
      "POST: <full body of the original Reddit post>\n",
      "```\n",
      "\n",
      "Your goal is to output **one single‑line TL;DR** that captures the essence of the entire post. Follow these rules precisely:\n",
      "\n",
      "1. **Word count** – The TL;DR must contain **exactly 25 words** (no more, no less).  \n",
      "2. **No line breaks** – Output must be a **single line**; never add newlines, bullets, or any other formatting.  \n",
      "3. **Core content** – Include the following elements in order of importance:\n",
      "   - **Main actors** (e.g., poster, partner, friend, pet, etc.).  \n",
      "   - **Central event or dilemma** (e.g., health issue, breakup, nagging, animal assistance).  \n",
      "   - **Emotional stakes or unresolved question** (e.g., “is the medication the cause?”, “how to respond?”, “is it a true sign of affection?”).  \n",
      "4. **Exclude extraneous material** – Omit URLs, Markdown, emojis, subreddit tags, “TL;DR:” prefixes, filler words, or any content that does not add meaning.  \n",
      "5. **Short posts** – If the original post is extremely short, use its full content as the TL;DR, trimming to 25 words if needed.  \n",
      "6. **Tone** – Keep the tone **neutral, factual, and succinct**.  \n",
      "7. **Generalizable strategy** – Use the following workflow for every input:\n",
      "   1. **Ignore everything before the “POST:” line.**  \n",
      "   2. **Identify the main actors** and their actions.  \n",
      "   3. **Spot the conflict or question** that drives the poster’s request for advice or explanation.  \n",
      "   4. **Condense** that information into a single, coherent sentence or clause.  \n",
      "   5. **Count words**; adjust phrasing to hit exactly 25 words.  \n",
      "   6. **Verify** there are no line breaks before submitting the TL;DR.\n",
      "\n",
      "**Deliverable**: Only the TL;DR text—no additional commentary, no surrounding markup, no explanation.\n",
      "2025/11/17 12:08:35 INFO dspy.evaluate.evaluate: Average Metric: 2.76210164030393 / 3 (92.1%)\n",
      "2025/11/17 12:08:35 INFO dspy.teleprompt.gepa.gepa: Iteration 6: New subsample score 2.76210164030393 is not better than old score 2.795271078745524, skipping\n",
      "GEPA Optimization:  88%|████████▊ | 1036/1180 [43:52<07:09,  2.98s/rollouts]2025/11/17 12:08:35 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Selected program 4 score: 0.9292012715287231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.79 / 3 (93.0%): 100%|██████████| 3/3 [00:09<00:00,  3.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 12:08:44 INFO dspy.evaluate.evaluate: Average Metric: 2.78872807820638 / 3 (93.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 12:08:51 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Proposed new text for generator.predict: SUBREDDIT: <subreddit name>\n",
      "TITLE: <post title>\n",
      "POST: <full body of the original Reddit post>\n",
      "2025/11/17 12:09:05 INFO dspy.evaluate.evaluate: Average Metric: 1.5088952625239336 / 3 (50.3%)\n",
      "2025/11/17 12:09:05 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New subsample score 1.5088952625239336 is not better than old score 2.78872807820638, skipping\n",
      "GEPA Optimization:  88%|████████▊ | 1042/1180 [44:22<06:57,  3.03s/rollouts]2025/11/17 12:09:05 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Selected program 3 score: 0.9144158045835793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.77 / 3 (92.5%): 100%|██████████| 3/3 [00:08<00:00,  2.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 12:09:14 INFO dspy.evaluate.evaluate: Average Metric: 2.7743208178767453 / 3 (92.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 12:09:19 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Proposed new text for generator.predict: You will receive a Reddit thread description that follows this exact format:\n",
      "\n",
      "```\n",
      "SUBREDDIT: <subreddit name>\n",
      "TITLE: <title of the post>\n",
      "POST: <full post text, possibly several paragraphs>\n",
      "```\n",
      "\n",
      "Your task is to generate **one single line** that is a concise TL;DR of the post.  \n",
      "The TL;DR must:\n",
      "\n",
      "1. **Capture the core meaning** of the post, including the key decision point, conflict, or question the poster is asking for help with.  \n",
      "2. **Be approximately 20‑30 words** (aim for ~25 words). If the generated line is shorter than 20 words or longer than 30 words, adjust by adding or removing **one descriptive word**.  \n",
      "3. **Be a single line** – do **not** insert line breaks, bullet points, or any additional formatting.  \n",
      "4. **Contain no extra commentary** – do not add your own thoughts, labels, or instructions.  \n",
      "5. **Read as a standalone summary** that a reader can understand without seeing the original post.  \n",
      "\n",
      "After producing the TL;DR, the assistant must not output any additional text.\n",
      "2025/11/17 12:09:27 INFO dspy.evaluate.evaluate: Average Metric: 2.8195076183036525 / 3 (94.0%)\n",
      "2025/11/17 12:09:27 INFO dspy.teleprompt.gepa.gepa: Iteration 8: New subsample score 2.8195076183036525 is better than old score 2.7743208178767453. Continue to full eval and add to candidate pool.\n",
      "2025/11/17 12:17:46 INFO dspy.evaluate.evaluate: Average Metric: 183.2968314871192 / 200 (91.6%)\n",
      "2025/11/17 12:17:46 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Full valset score for new program: 0.916484157435596\n",
      "2025/11/17 12:17:46 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Full train_val score for new program: 0.916484157435596\n",
      "2025/11/17 12:17:46 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Individual valset scores for new program: [0.8716152685659903, 0.9219022781760605, 0.8884697340152882, 0.929224510104568, 0.8792834905562578, 0.938457578420639, 0.9076589531368681, 0.9340492776146642, 0.8839028563764361, 0.8907409983652609, 0.8016013171385836, 0.8945813856743, 0.9465463351320338, 0.8768446924509825, 0.9266405895904258, 0.8789998076580189, 0.925400643657755, 0.9243340289151227, 0.9020208409538976, 0.9486444195111593, 0.9246828456719717, 0.9330499763841983, 0.9253149425541913, 0.9133633569434837, 0.9170197442725853, 0.8941527483639894, 0.8975676975868366, 0.9148574151374675, 0.8720199104812411, 0.9013351491204015, 0.954532821531649, 0.9204037569187306, 0.9342018365859985, 0.9201513519993535, 0.9008059104283651, 0.8757111408092357, 0.9064654157117561, 0.9190157146365555, 0.9276040092662529, 0.9118095519366087, 0.8904594955620942, 0.9195939496711448, 0.8898803569652416, 0.8914594794865008, 0.9257694564483784, 0.9126205638602928, 0.9500588264730242, 0.898099501044662, 0.9454249540964762, 0.9510021805763245, 0.9362414366669126, 0.9219974279403687, 0.887466021158077, 0.9079099478545012, 0.9106774519990992, 0.8910818289827418, 0.9125182421119126, 0.8983858603018301, 0.942357102588371, 0.9266963199332908, 0.8640558747229753, 0.9390994301548711, 0.9323952482806313, 0.8469463366049307, 0.946444590444918, 0.9231019020080566, 0.9249585862512942, 0.9216462762267502, 0.91958087682724, 0.9156577172102751, 0.9146148648526934, 0.950579305489858, 0.9659738734916404, 0.918727218663251, 0.9405293857609784, 0.9169677970586, 0.939850448679041, 0.9263914123729423, 0.9363203920699932, 0.8828639076815712, 0.9013916592907023, 0.9164966841538748, 0.8924919859126762, 0.927545546160804, 0.9516831437746683, 0.9173297277203313, 0.9054903975239507, 0.9025049904982249, 0.9652937153975168, 0.8803295132186678, 0.8733826961782244, 0.8841064252235271, 0.9096224104916608, 0.9208870123933863, 0.8957210621348134, 0.887363957254975, 0.9423420329888662, 0.9273324496216245, 0.9051880290110906, 0.9187507028932925, 0.9188488112555611, 0.9322663143829063, 0.8861311751383322, 0.8929651668778172, 0.8998402235684572, 0.9089528512071681, 0.9155090045045924, 0.9092045020174098, 0.9450427889823914, 0.9264567874096058, 0.9258343952673452, 0.9414885527557798, 0.9387559683234604, 0.9229537036683825, 0.9144694593217638, 0.9166420693750735, 0.9407713011459068, 0.9351388012921369, 0.9414271509205854, 0.9053417227886341, 0.9301003416379293, 0.9149896895443952, 0.9353952295250364, 0.9341306081524602, 0.9280817305600202, 0.9516394729967471, 0.9587112263396934, 0.9008652550202828, 0.8929216049335621, 0.9085390613034919, 0.9320130836080622, 0.9292126090438279, 0.9053358882665634, 0.8899065147947383, 0.9394512167683354, 0.8874329080184301, 0.9427280024245933, 0.9304472695898127, 0.9445687428668693, 0.923207739547447, 0.939223348652875, 0.8919928250489412, 0.9276862329906889, 0.8681452952049397, 0.8911772704786726, 0.9331736167271932, 0.9138887804967385, 0.9433638453483582, 0.9195113078311637, 0.9520376225312551, 0.9134960170145389, 0.8707412170039284, 0.9334336316144025, 0.9326970076119458, 0.9442610239541089, 0.9288715024789175, 0.8910540925131905, 0.9271377022619601, 0.9057069407569038, 0.9124901475729765, 0.9000354788921497, 0.8408239656024509, 0.8972221500343748, 0.9167234879952889, 0.9341746866703033, 0.8885211791153308, 0.9313629161428523, 0.9511015212094343, 0.9282107737329272, 0.880836902282856, 0.8953109672775975, 0.8850511109387433, 0.9338382378772453, 0.9432733451878583, 0.89122925610454, 0.9004115440227367, 0.8976663346643802, 0.9202072002269603, 0.9228110891801339, 0.924582858880361, 0.9280998598646235, 0.9190842509269714, 0.894081582625707, 0.9547480539039329, 0.9308298722461418, 0.9066401268597003, 0.946748693784078, 0.8980422954868388, 0.9480587939421335, 0.9263897533769961, 0.9325377539352134, 0.954287032286326, 0.895863174950635, 0.9117741377265366, 0.9361956201217793, 0.9094311391865766, 0.9632375439008077, 0.9515405694643656, 0.9457586705684662, 0.9256348899117223]\n",
      "2025/11/17 12:17:46 INFO dspy.teleprompt.gepa.gepa: Iteration 8: New valset pareto front scores: [0.9150915543238322, 0.943981796503067, 0.908057509969782, 0.9353291789690653, 0.9055757414411616, 0.938457578420639, 0.9366672039031982, 0.9533300995826721, 0.9197090665499369, 0.9391659696896871, 0.8222222222222223, 0.9531382814601616, 0.9631000359853109, 0.9125821382911118, 0.9603974123795828, 0.9090010907914904, 0.9553695519765218, 0.937074601650238, 0.9563838044802347, 0.9499673843383789, 0.9246828456719717, 0.9463385537818626, 0.939671297426577, 0.9518270492553711, 0.9235762750660932, 0.9506438473860422, 0.939069668451945, 0.9637969533602396, 0.9154735207557678, 0.9308399856090546, 0.9614360531171163, 0.9563544789950053, 0.9342018365859985, 0.943115254243215, 0.9340404868125916, 0.9542914231618246, 0.9305095771948496, 0.9314848979314169, 0.9276040092662529, 0.9406250516573588, 0.9691428144772848, 0.9250842626448031, 0.8920850604772568, 0.936798503001531, 0.958245575428009, 0.9253091812133789, 0.9710431694984436, 0.9319363832473755, 0.9486217991069511, 0.9510021805763245, 0.9571031729380289, 0.9219974279403687, 0.9360397656758627, 0.924395372028704, 0.9346660772959391, 0.9221192991292035, 0.9348098631258365, 0.9154327209349032, 0.942357102588371, 0.9266963199332908, 0.9198993543783823, 0.9407496745939609, 0.9323952482806313, 0.9143519794499433, 0.946444590444918, 0.9267865911678032, 0.9329632918039957, 0.93843146165212, 0.9596984287103018, 0.9327459136644999, 0.9451556404431661, 0.950579305489858, 0.9704910814762115, 0.918727218663251, 0.9408947030703226, 0.9493109778121666, 0.9549164171572085, 0.9263914123729423, 0.9654320081075033, 0.91246530910333, 0.9165140787760416, 0.9164966841538748, 0.9112384070952734, 0.9496792852878571, 0.9565864503383636, 0.9375701149304708, 0.9258680939674377, 0.9130295212622043, 0.9652937153975168, 0.9416347940762838, 0.9191041191418966, 0.913228174050649, 0.9448617891029075, 0.9468572537104288, 0.9111983974774679, 0.9342001469046982, 0.9586756328741709, 0.9346562127272288, 0.9128720859686533, 0.9338264862696329, 0.9522727827231089, 0.9322663143829063, 0.9254104693730673, 0.9343900084495544, 0.926751454671224, 0.93491197625796, 0.9411555230617523, 0.9356292680457786, 0.9486519495646158, 0.9264567874096058, 0.9486787915229797, 0.9414885527557798, 0.9525721867879232, 0.9354092677434286, 0.9378829995791117, 0.930656929369326, 0.9407713011459068, 0.9511248663619712, 0.9483305513858795, 0.9567080438137054, 0.9430648982524872, 0.9520266254742941, 0.9725002845128378, 0.9504059851169586, 0.9436426948618006, 0.9566758871078491, 0.9718090693155924, 0.9309698442618052, 0.9087053388357162, 0.9263331194718679, 0.9536926547686259, 0.9292126090438279, 0.9497138460477194, 0.9161428407386497, 0.9536293645699819, 0.9063472400108973, 0.9427280024245933, 0.9533597528934479, 0.9579155544439951, 0.923207739547447, 0.9532651702562968, 0.9531590541203817, 0.9314551249698356, 0.9163979142904282, 0.9372117817401886, 0.9331736167271932, 0.9219654103120168, 0.9462265173594157, 0.9583107829093933, 0.9646257162094116, 0.9134960170145389, 0.9301813244819641, 0.948601504166921, 0.9364165663719177, 0.9442610239541089, 0.9541164338588715, 0.9161527355511984, 0.9453115260159528, 0.9593089818954468, 0.9446131587028503, 0.9402421613534292, 0.934287448282595, 0.9304213817472812, 0.9284064259794024, 0.9424540201822916, 0.9274828926280693, 0.9335941076278687, 0.9578128655751547, 0.9686029752095541, 0.9023476234188786, 0.9118584977255928, 0.9545264740784963, 0.9477193355560303, 0.9453300233240481, 0.925552158664774, 0.9306105573972067, 0.9511249562104543, 0.9583057661851248, 0.9540700813134512, 0.949586808681488, 0.9280998598646235, 0.921850343103762, 0.9168568240271675, 0.9578947921593984, 0.9372591177622477, 0.9367205500602722, 0.9505934615929922, 0.9217786585843122, 0.9480587939421335, 0.9318521718184153, 0.932659775018692, 0.954287032286326, 0.9212917667848092, 0.9117741377265366, 0.960391084353129, 0.9221810102462769, 0.9632375439008077, 0.9520653088887533, 0.9457586705684662, 0.9543858364776329]\n",
      "2025/11/17 12:17:46 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Full valset pareto front score: 0.9378824021429928\n",
      "2025/11/17 12:17:46 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Updated valset pareto front programs: [{4}, {4}, {0}, {4}, {1}, {5}, {1}, {4}, {4}, {4}, {1, 2}, {1}, {4}, {2}, {4}, {3}, {0}, {4}, {3}, {2}, {5}, {3}, {0}, {4}, {1}, {4}, {3}, {0}, {3}, {4}, {4}, {1}, {5}, {4}, {4}, {3}, {0}, {4}, {5}, {3}, {1}, {1}, {4}, {4}, {1}, {1}, {0}, {4}, {1}, {5}, {4}, {5}, {1}, {2}, {4}, {3}, {1}, {0}, {5}, {5}, {4}, {3}, {5}, {0}, {5}, {0}, {0}, {4}, {1}, {4}, {1}, {5}, {4}, {5}, {4}, {3}, {1}, {5}, {1}, {4}, {4}, {5}, {2}, {4}, {4}, {4}, {4}, {0}, {5}, {2}, {2}, {4}, {3}, {2}, {4}, {1}, {1}, {4}, {1}, {2}, {4}, {5}, {4}, {2}, {4}, {4}, {1}, {1}, {0}, {5}, {4}, {5}, {4}, {1}, {4}, {0}, {5}, {0}, {4}, {4}, {0}, {4}, {4}, {2}, {0}, {4}, {2}, {4}, {4}, {4}, {1}, {5}, {4}, {0}, {4}, {3}, {5}, {4}, {0}, {5}, {4}, {4}, {1}, {4}, {4}, {5}, {4}, {4}, {2}, {4}, {5}, {4}, {0}, {4}, {5}, {4}, {4}, {3}, {2}, {0}, {4}, {2}, {2}, {1}, {1}, {0}, {4}, {4}, {2}, {3}, {0}, {1}, {1}, {1}, {3}, {4}, {1}, {4}, {1}, {4}, {5}, {1}, {3}, {4}, {4}, {3}, {3}, {1}, {5}, {4}, {4}, {5}, {3}, {5}, {4}, {1}, {5}, {4}, {5}, {2}]\n",
      "2025/11/17 12:17:46 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Best valset aggregate score so far: 0.9292012715287231\n",
      "2025/11/17 12:17:46 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Best program as per aggregate score on train_val: 4\n",
      "2025/11/17 12:17:46 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Best program as per aggregate score on valset: 4\n",
      "2025/11/17 12:17:46 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Best score on valset: 0.9292012715287231\n",
      "2025/11/17 12:17:46 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Best score on train_val: 0.9292012715287231\n",
      "2025/11/17 12:17:46 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Linear pareto front program index: 4\n",
      "2025/11/17 12:17:46 INFO dspy.teleprompt.gepa.gepa: Iteration 8: New program candidate index: 5\n",
      "GEPA Optimization:  88%|████████▊ | 1042/1180 [53:03<07:01,  3.06s/rollouts]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 7) GEPA OPTIMIZER\n",
    "# ============================================================\n",
    "from dspy import GEPA\n",
    "\n",
    "optimizer = GEPA(\n",
    "    metric=metric_with_feedback,\n",
    "    auto=\"light\",\n",
    "    num_threads=16,\n",
    "    track_stats=True,\n",
    "    track_best_outputs=True,\n",
    "    use_merge=False,\n",
    "    reflection_lm=reflection_lm\n",
    ")\n",
    "\n",
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=train_set,\n",
    "    valset=val_set,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81f77028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "Predictor: generator.predict\n",
      "================================\n",
      "Prompt:\n",
      "**TL;DR Generation for Reddit Threads**\n",
      "\n",
      "You will receive a Reddit thread excerpt that always follows this exact format:\n",
      "\n",
      "```\n",
      "SUBREDDIT: <subreddit name>\n",
      "TITLE: <post title>\n",
      "POST: <full body of the original Reddit post>\n",
      "```\n",
      "\n",
      "Your task is to produce a **single‑line TL;DR** that summarizes the entire post. Follow these rules:\n",
      "\n",
      "1. **Length** – The TL;DR must contain **exactly 25 words** (no more, no less).  \n",
      "2. **Line breaks** – The output must be a **single line**; no newlines, bullets, or other formatting.  \n",
      "3. **Content** – Capture the core narrative or conflict:\n",
      "   * Main actors (e.g., poster, partner, friend).  \n",
      "   * Central event or dilemma (e.g., health issue, breakup, sabotage).  \n",
      "   * Emotional stakes or unresolved question (e.g., “is the medication the cause?”).  \n",
      "4. **What to exclude** – Omit URLs, markdown, emojis, extraneous tags, and any filler words that do not add meaning.  \n",
      "5. **If the post is extremely short** – Use the entire content as the TL;DR, but trim it to 25 words if necessary.  \n",
      "6. **Tone** – Neutral, factual, and succinct.  \n",
      "7. **Generalizable strategy**  \n",
      "   1. Ignore everything before the “POST:” line.  \n",
      "   2. Identify the main actors and their actions.  \n",
      "   3. Spot the conflict or question driving the post.  \n",
      "   4. Condense that information into a single, coherent sentence or clause.  \n",
      "   5. Count words; adjust phrasing to hit 25 words exactly.  \n",
      "   6. Verify there are no line breaks before submitting the TL;DR.  \n",
      "\n",
      "**Deliver only the TL;DR text—nothing else.**\n",
      "*********************************\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8) SEE OPTIMIZED PROMPTS\n",
    "# ============================================================\n",
    "for name, pred in optimized_program.named_predictors():\n",
    "    print(\"================================\")\n",
    "    print(f\"Predictor: {name}\")\n",
    "    print(\"================================\")\n",
    "    print(\"Prompt:\")\n",
    "    print(pred.signature.instructions)\n",
    "    print(\"*********************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00524f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== GEPA-Optimized Evaluation ==\n",
      "Average Metric: 186.43 / 200 (93.2%): 100%|██████████| 200/200 [12:13<00:00,  3.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/17 12:29:59 INFO dspy.evaluate.evaluate: Average Metric: 186.425146161075 / 200 (93.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>gold_tldr</th>\n",
       "      <th>tldr</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUBREDDIT: r/relationships TITLE: Me [16 F] with my boyfriend [18 ...</td>\n",
       "      <td>I get too worried when my bf goes out. I need help so I make it s...</td>\n",
       "      <td>16‑year‑old girlfriend feels overly attached to 18‑year‑old boyfri...</td>\n",
       "      <td>✔️ [0.890]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SUBREDDIT: r/AskReddit TITLE: Indirect Demoralization at work POST...</td>\n",
       "      <td>went up to ask for raise, got shut down before even asking, now I...</td>\n",
       "      <td>Year without raise and duties, I planned to ask, but boss’s dispar...</td>\n",
       "      <td>✔️ [0.916]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUBREDDIT: r/relationships TITLE: My boyfriend [21] hasn't made me...</td>\n",
       "      <td>Boyf hasnt made me orgasm, I've been faking the whole time. To te...</td>\n",
       "      <td>18‑year‑old woman never orgasmed with 21‑year‑old boyfriend after ...</td>\n",
       "      <td>✔️ [0.924]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SUBREDDIT: r/relationships TITLE: I [22F] matched with an ex [23M]...</td>\n",
       "      <td>Matched w a guy I used to date on tinder. Would like to talk to h...</td>\n",
       "      <td>22‑year‑old woman, previously dating ex‑boyfriend, unexpectedly ma...</td>\n",
       "      <td>✔️ [0.934]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SUBREDDIT: r/relationship_advice TITLE: [19/m] where do I go from ...</td>\n",
       "      <td>talking to girl, not sure if she feels the same way as I do, but ...</td>\n",
       "      <td>19‑year‑old male wonders if 18‑year‑old female friend wants a roma...</td>\n",
       "      <td>✔️ [0.915]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>SUBREDDIT: r/dating_advice TITLE: Where Does a Friend-Zone/Relatio...</td>\n",
       "      <td>! How do I know if she wants to be in the friend-zone? How do I k...</td>\n",
       "      <td>Seventeen‑year‑old poster wonders if his 16‑year‑old friend, with ...</td>\n",
       "      <td>✔️ [0.911]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>SUBREDDIT: r/AskReddit TITLE: Should I consider going back to Digg...</td>\n",
       "      <td>Reddit is fading, especially with the lag..to the Digg user base!...</td>\n",
       "      <td>I loved Reddit’s fast, well‑organized front page like Digg’s, but ...</td>\n",
       "      <td>✔️ [0.968]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>SUBREDDIT: r/relationships TITLE: Boyfriend [25m] broke up with me...</td>\n",
       "      <td>Boyfriend of 3 1/2 years broke up with me and I can't move out fo...</td>\n",
       "      <td>I broke up with my 25M boyfriend; we live together, I must continu...</td>\n",
       "      <td>✔️ [0.908]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>SUBREDDIT: r/tifu TITLE: TIFU by ruining my best friends collegiat...</td>\n",
       "      <td>Roundhoused my friend in the hip, he fell and fucked up his ankle...</td>\n",
       "      <td>I attempted to tap my friend Ryan, causing him to fall, break his ...</td>\n",
       "      <td>✔️ [0.939]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>SUBREDDIT: r/needadvice TITLE: Reddit: How can I get over the ment...</td>\n",
       "      <td>My erections are 90% mental and I can't get/keep them up outside ...</td>\n",
       "      <td>25-year-old male with general impotence fears only resolves during...</td>\n",
       "      <td>✔️ [0.927]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    prompt  \\\n",
       "0    SUBREDDIT: r/relationships TITLE: Me [16 F] with my boyfriend [18 ...   \n",
       "1    SUBREDDIT: r/AskReddit TITLE: Indirect Demoralization at work POST...   \n",
       "2    SUBREDDIT: r/relationships TITLE: My boyfriend [21] hasn't made me...   \n",
       "3    SUBREDDIT: r/relationships TITLE: I [22F] matched with an ex [23M]...   \n",
       "4    SUBREDDIT: r/relationship_advice TITLE: [19/m] where do I go from ...   \n",
       "..                                                                     ...   \n",
       "195  SUBREDDIT: r/dating_advice TITLE: Where Does a Friend-Zone/Relatio...   \n",
       "196  SUBREDDIT: r/AskReddit TITLE: Should I consider going back to Digg...   \n",
       "197  SUBREDDIT: r/relationships TITLE: Boyfriend [25m] broke up with me...   \n",
       "198  SUBREDDIT: r/tifu TITLE: TIFU by ruining my best friends collegiat...   \n",
       "199  SUBREDDIT: r/needadvice TITLE: Reddit: How can I get over the ment...   \n",
       "\n",
       "                                                                 gold_tldr  \\\n",
       "0     I get too worried when my bf goes out. I need help so I make it s...   \n",
       "1     went up to ask for raise, got shut down before even asking, now I...   \n",
       "2     Boyf hasnt made me orgasm, I've been faking the whole time. To te...   \n",
       "3     Matched w a guy I used to date on tinder. Would like to talk to h...   \n",
       "4     talking to girl, not sure if she feels the same way as I do, but ...   \n",
       "..                                                                     ...   \n",
       "195   ! How do I know if she wants to be in the friend-zone? How do I k...   \n",
       "196   Reddit is fading, especially with the lag..to the Digg user base!...   \n",
       "197   Boyfriend of 3 1/2 years broke up with me and I can't move out fo...   \n",
       "198   Roundhoused my friend in the hip, he fell and fucked up his ankle...   \n",
       "199   My erections are 90% mental and I can't get/keep them up outside ...   \n",
       "\n",
       "                                                                      tldr  \\\n",
       "0    16‑year‑old girlfriend feels overly attached to 18‑year‑old boyfri...   \n",
       "1    Year without raise and duties, I planned to ask, but boss’s dispar...   \n",
       "2    18‑year‑old woman never orgasmed with 21‑year‑old boyfriend after ...   \n",
       "3    22‑year‑old woman, previously dating ex‑boyfriend, unexpectedly ma...   \n",
       "4    19‑year‑old male wonders if 18‑year‑old female friend wants a roma...   \n",
       "..                                                                     ...   \n",
       "195  Seventeen‑year‑old poster wonders if his 16‑year‑old friend, with ...   \n",
       "196  I loved Reddit’s fast, well‑organized front page like Digg’s, but ...   \n",
       "197  I broke up with my 25M boyfriend; we live together, I must continu...   \n",
       "198  I attempted to tap my friend Ryan, causing him to fall, break his ...   \n",
       "199  25-year-old male with general impotence fears only resolves during...   \n",
       "\n",
       "         metric  \n",
       "0    ✔️ [0.890]  \n",
       "1    ✔️ [0.916]  \n",
       "2    ✔️ [0.924]  \n",
       "3    ✔️ [0.934]  \n",
       "4    ✔️ [0.915]  \n",
       "..          ...  \n",
       "195  ✔️ [0.911]  \n",
       "196  ✔️ [0.968]  \n",
       "197  ✔️ [0.908]  \n",
       "198  ✔️ [0.939]  \n",
       "199  ✔️ [0.927]  \n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(score=93.21, results=<list of 200 results>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9) FINAL POST-GEPA EVALUATION\n",
    "# ============================================================\n",
    "print(\"\\n== GEPA-Optimized Evaluation ==\")\n",
    "evaluate(optimized_program)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21891a68",
   "metadata": {},
   "source": [
    "## Performance Improvement Summary\n",
    "\n",
    "Display the accuracy lift from baseline (90.92%) to optimized (93.21%).\n",
    "\n",
    "### Comparison: GEPA vs MIPROv2\n",
    "\n",
    "| Optimizer | Baseline | Optimized | Improvement |\n",
    "|-----------|----------|-----------|-------------|\n",
    "| GEPA | 90.92% | 93.21% | +2.29 pp |\n",
    "| MIPROv2 | 90.92% | 91.13% | +0.21 pp |\n",
    "\n",
    "*pp = percentage points*  \n",
    "Corresponding MIPROv2 results are documented in `miprov2_3_tldr.ipynb`. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
