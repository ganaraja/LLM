{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec841000",
   "metadata": {},
   "source": [
    "# **DSPy Optimization**\n",
    "\n",
    "## **1. What is a DSPy Optimizer?**\n",
    "\n",
    "A **DSPy optimizer** is an algorithm that *automatically tunes the parameters of a DSPy program* — including:\n",
    "\n",
    "* Prompt instructions\n",
    "* Few-shot examples\n",
    "* Module configurations\n",
    "* LM weights (optional)\n",
    "\n",
    "The goal is to **maximize a metric** such as accuracy, F1, or exact match.\n",
    "\n",
    "---\n",
    "\n",
    "## An optimizer needs:\n",
    "\n",
    "1. **Your DSPy Program**\n",
    "   (single module like `Predict`, or a full multi-step pipeline)\n",
    "2. **Metric Function**\n",
    "   (e.g., `dspy.evaluate.answer_exact_match`)\n",
    "3. **Training Inputs**\n",
    "\n",
    "   * Can be **very small** (even 5–10 examples)\n",
    "   * May include only inputs, or inputs + labels\n",
    "\n",
    "DSPy optimizers are composable — you can run one optimizer on top of another.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. What Does a DSPy Optimizer Tune?**\n",
    "\n",
    "Depending on the optimizer type, DSPy will tune:\n",
    "\n",
    "### ✔ **Few-Shot Examples**\n",
    "\n",
    "* Generate high-quality demonstration examples\n",
    "* Filter them using your metric\n",
    "* Insert them into module prompts\n",
    "\n",
    "### ✔ **Prompt Instructions**\n",
    "\n",
    "* Rewrite natural-language instructions for each module\n",
    "* Improve clarity, constraints, and task definitions\n",
    "\n",
    "\n",
    "### ✔ **LM Weights** (finetuning)\n",
    "\n",
    "* Convert a prompt-based pipeline into a **finetuned model**\n",
    "* Improves efficiency and stability\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Categories of DSPy Optimizers**\n",
    "\n",
    "\n",
    "# **A. Automatic Few-Shot Learning**\n",
    "\n",
    "These optimizers **extend prompts** by inserting automatically generated few-shot examples.\n",
    "\n",
    "### **1. LabeledFewShot**\n",
    "\n",
    "* Uses *provided* labeled examples only.\n",
    "* Simply picks **k random examples** from your dataset.\n",
    "* Good for rapid prototyping.\n",
    "\n",
    "**When to use:**\n",
    "Small, clean labeled dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. BootstrapFewShot**\n",
    "\n",
    "* Uses your program (teacher) to **generate additional demonstrations**.\n",
    "* Combines:\n",
    "\n",
    "  * Labeled examples\n",
    "  * Bootstrapped examples\n",
    "* Keeps only *metric-approved* demos.\n",
    "\n",
    "**Key parameters:**\n",
    "\n",
    "* `max_labeled_demos`\n",
    "* `max_bootstrapped_demos`\n",
    "\n",
    "**When to use:**\n",
    "Very small training data (about 10 examples).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. BootstrapFewShotWithRandomSearch**\n",
    "\n",
    "* Repeats BootstrapFewShot multiple times.\n",
    "* Randomizes order and selection of demos.\n",
    "* Evaluates all candidate programs and picks the best.\n",
    "\n",
    "**Key parameter:**\n",
    "\n",
    "* `num_candidate_programs` (e.g. 10–50)\n",
    "\n",
    "**When to use:**\n",
    "Moderate-sized data (50+ examples) and want better robustness.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **B. Automatic Instruction Optimization**\n",
    "\n",
    "These optimize instructions inside each module’s prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. MIPROv2**\n",
    "\n",
    "* Optimizes **both instructions + demos**.\n",
    "* Three-stage process:\n",
    "\n",
    "  1. **Bootstrapping** → collect program traces\n",
    "  2. **Grounded proposal** → generate new instructions using code + traces\n",
    "  3. **Bayesian optimization** → explore instruction/demonstration combinations\n",
    "\n",
    "\n",
    "**When to use:**\n",
    "Serious optimization on medium/large tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. GEPA (Reflective Prompt Evolution)**\n",
    "\n",
    "* Program-level reflection:\n",
    "\n",
    "  * Identifies what worked / failed\n",
    "  * Improves prompts with meta reasoning\n",
    "* Can ingest **domain-specific feedback documents**\n",
    "\n",
    "**When to use:**\n",
    "Enterprise tasks, specialized reasoning pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "# **C. Automatic Finetuning**\n",
    "\n",
    "### **BootstrapFinetune**\n",
    "\n",
    "* Distills your optimized DSPy prompts into **model weights**.\n",
    "* Produces a program where every module uses a **finetuned LM** instead of prompting.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "* Faster inference\n",
    "* Lower cost\n",
    "* Greater consistency\n",
    "\n",
    "**When to use:**\n",
    "Production-grade or edge-deployable models.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Which Optimizer Should You Use?**\n",
    "\n",
    "| Scenario                                 | Recommended Optimizer                |\n",
    "| ---------------------------------------- | ------------------------------------ |\n",
    "| Very few examples (≈10)                  | **BootstrapFewShot**                 |\n",
    "| Medium dataset (50+)                     | **BootstrapFewShotWithRandomSearch** |\n",
    "| Want 0-shot prompts (no demos)           | **MIPROv2 (0-shot mode)**            |\n",
    "| Large dataset (200+) + long optimization | **MIPROv2**                          |\n",
    "| Want reflective, self-evolving prompts   | **GEPA**                             |\n",
    "| Want finetuned weights (small model)     | **BootstrapFinetune**                |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
