{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38a434d6",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://supportvectors.ai/logo-poster-transparent.png\" width=\"400px\" style=\"opacity:0.7\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab180347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!-- Many of the styles here are inspired by: \n",
       "    https://towardsdatascience.com/10-practical-tips-you-need-to-know-to-personalize-jupyter-notebook-fbd202777e20 \n",
       "\n",
       "\n",
       "    On the author's local machine, these exist in the custom.css file. However, in order to keep uniform look and feel, \n",
       "    and at the request of participants, I have added it to this common import-file here.\n",
       "\n",
       "    -->\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Lora:400,700|Montserrat:300\" rel=\"stylesheet\">\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Crimson+Pro&family=Literata&display=swap\" rel=\"stylesheet\">\n",
       "<style>\n",
       "\n",
       "\n",
       "#ipython_notebook::before{\n",
       " content:\"Neural Architectures\";\n",
       "        color: white;\n",
       "        font-weight: bold;\n",
       "        text-transform: uppercase;\n",
       "        font-family: 'Lora',serif;\n",
       "        font-size:16pt;\n",
       "        margin-bottom:15px;\n",
       "        margin-top:15px;\n",
       "\n",
       "}\n",
       "body > #header {\n",
       "    #background: #D15555;\n",
       "    background: linear-gradient(to bottom, indianred 0%, #fff 100%);\n",
       "    opacity: 0.8;\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       ".navbar-default .navbar-nav > li > a, #kernel_indicator {\n",
       "    color: white;\n",
       "    transition: all 0.25s;\n",
       "    font-size:10pt;\n",
       "    font-family: sans-serif;\n",
       "    font-weight:normal;\n",
       "}\n",
       ".navbar-default {\n",
       "    padding-left:100px;\n",
       "    background: none;\n",
       "    border: none;\n",
       "}\n",
       "\n",
       "\n",
       "body > menubar-container {\n",
       "    background-color: wheat;\n",
       "}\n",
       "#ipython_notebook img{                                                                                        \n",
       "    display:block; \n",
       "\n",
       "    background: url(\"https://www.supportvectors.com/wp-content/uploads/2016/03/logo-poster-smaller.png\") no-repeat;\n",
       "    background-size: contain;\n",
       "\n",
       "    padding-left: 600px;\n",
       "    padding-right: 100px;\n",
       "\n",
       "    -moz-box-sizing: border-box;\n",
       "    box-sizing: border-box;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "body {\n",
       " #font-family:  'Literata', serif;\n",
       "    font-family:'Lora', san-serif;\n",
       "    text-align: justify;\n",
       "    font-weight: 400;\n",
       "    font-size: 12pt;\n",
       "}\n",
       "\n",
       "iframe{\n",
       "    width:100%;\n",
       "    min-height:600px;\n",
       "}\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "# font-family: 'Montserrat', sans-serif;\n",
       " font-family:'Lora', serif;\n",
       " font-weight: 200;\n",
       " text-transform: uppercase;\n",
       " color: #EC7063 ;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    color: #000080;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       "#notebook_name {\n",
       "    font-weight: 600;\n",
       "    font-size:20pt;\n",
       "    text-variant:uppercase;\n",
       "    color: wheat; \n",
       "    margin-right:20px;\n",
       "    margin-left:-500px;\n",
       "}\n",
       "#notebook_name:hover {\n",
       "background-color: salmon;\n",
       "}\n",
       "\n",
       "\n",
       ".dataframe { /* dataframe atau table */\n",
       "    background: white;\n",
       "    box-shadow: 0px 1px 2px #bbb;\n",
       "}\n",
       ".dataframe thead th, .dataframe tbody td {\n",
       "    text-align: center;\n",
       "    padding: 1em;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       ".output {\n",
       "    align-items: center;\n",
       "}\n",
       "\n",
       "div.cell {\n",
       "    transition: all 0.25s;\n",
       "    border: none;\n",
       "    position: relative;\n",
       "    top: 0;\n",
       "}\n",
       "div.cell.selected, div.cell.selected.jupyter-soft-selected {\n",
       "    border: none;\n",
       "    background: transparent;\n",
       "    box-shadow: 0 6px 18px #aaa;\n",
       "    z-index: 10;\n",
       "    top: -10px;\n",
       "}\n",
       ".CodeMirror pre, .CodeMirror-dialog, .CodeMirror-dialog .CodeMirror-search-field, .terminal-app .terminal {\n",
       "    font-family: 'Hack' , serif; \n",
       "    font-weight: 500;\n",
       "    font-size: 14pt;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "</style>    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<div style=\"color:#aaa;font-size:8pt\">\n",
       "<hr/>\n",
       "&copy; SupportVectors. All rights reserved. <blockquote>This notebook is the intellectual property of SupportVectors, and part of its training material. \n",
       "Only the participants in SupportVectors workshops are allowed to study the notebooks for educational purposes currently, but is prohibited from copying or using it for any other purposes without written permission.\n",
       "\n",
       "<b> These notebooks are chapters and sections from Asif Qamar's textbook that he is writing on Data Science. So we request you to not circulate the material to others.</b>\n",
       " </blockquote>\n",
       " <hr/>\n",
       "</div>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86073098",
   "metadata": {},
   "source": [
    "# Chunking Pipeline\n",
    "Here, we explore the chunking pipeline starting with chunking along section boundaries first, then breaking further into semantic chunks, and finally creating contextualized chunks using an LLM or leveraging late interactions to perform \"late chunking\" to bring in any context that may have been lost during the semantic chunking process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b36de",
   "metadata": {},
   "source": [
    "## Document ingestion → load document (PDF, Word, markdown).\n",
    "Use Docling to parse & chunk into parent chunks (each section/chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e130c811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 20:57:56,570 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-10-21 20:57:56,578 - INFO - Going to convert document batch...\n",
      "2025-10-21 20:57:56,579 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 4f2edc0f7d9bb60b38ebfecf9a2609f5\n",
      "2025-10-21 20:57:56,585 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-21 20:57:56,586 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-10-21 20:57:56,590 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-21 20:57:56,592 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-10-21 20:57:56,756 - INFO - Auto OCR model selected ocrmac.\n",
      "2025-10-21 20:57:56,767 - INFO - Accelerator device: 'mps'\n",
      "2025-10-21 20:57:57,930 - INFO - Accelerator device: 'mps'\n",
      "2025-10-21 20:57:58,075 - INFO - Processing document technicalReport1.pdf\n",
      "2025-10-21 20:58:03,162 - INFO - Finished converting document technicalReport1.pdf in 6.59 sec.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (706 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker\n",
    "import os\n",
    "doc = DocumentConverter().convert(source=f\"{os.getenv(\"BOOTCAMP_ROOT_DIR\")}/data/technicalReport1.pdf\").document\n",
    "chunker = HybridChunker()\n",
    "\n",
    "chunks = list(chunker.chunk(dl_doc=doc))\n",
    "parent_chunks = list(chunker.chunk(doc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdfe96a",
   "metadata": {},
   "source": [
    "## Semantic chunking \n",
    "Use `chonkie` to further semantically chunk each of above `parent` chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b72203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 20:58:07,947 - INFO - No cached model found for minishlab/potion-base-32M, loading from local or hub.\n",
      "2025-10-21 20:58:07,947 - INFO - Folder does not exist locally, attempting to use huggingface hub.\n"
     ]
    }
   ],
   "source": [
    "from chonkie import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker() # Use defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85547dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_chunks = [] # A running list of all semantic chunks (used during contextual chunking)\n",
    "parent_chunk_id = 0\n",
    "parent_chunk_dict_list = [] # A running list of all parent chunks (used during late chunking)\n",
    "\n",
    "for p in parent_chunks:\n",
    "    parent_chunk_dict = {}\n",
    "    parent_chunk_dict[\"id\"] = parent_chunk_id\n",
    "    parent_chunk_dict[\"text\"] = p.text\n",
    "    semantic_chunk_dict_list = []\n",
    "    parent_chunk_dict[\"semantic_chunks\"] = semantic_chunk_dict_list\n",
    "    parent_chunk_dict_list.append(parent_chunk_dict)\n",
    "    sem_chunks = semantic_chunker.chunk(p.text)\n",
    "    semantic_chunk_id = 0\n",
    "    for sc in sem_chunks:\n",
    "        # Maintain a list of semantic chunks for each parent chunk\n",
    "        semantic_chunk_dict = {}\n",
    "        semantic_chunk_dict[\"id\"] = semantic_chunk_id\n",
    "        semantic_chunk_dict[\"text\"] = sc.text\n",
    "        semantic_chunk_dict[\"start_char\"] = sc.start_index\n",
    "        semantic_chunk_dict[\"end_char\"] = sc.end_index\n",
    "        semantic_chunk_dict_list.append(semantic_chunk_dict)\n",
    "\n",
    "        # Maintain a list of all semantic chunks with their parent chunk\n",
    "        semantic_chunk = {}\n",
    "        semantic_chunk[\"chunk_id\"] = semantic_chunk_id\n",
    "        semantic_chunk[\"chunk\"] = sc\n",
    "        semantic_chunk[\"parent_id\"] = parent_chunk_id\n",
    "        semantic_chunk[\"parent_chunk\"] = p\n",
    "        semantic_chunks.append(semantic_chunk)\n",
    "\n",
    "        semantic_chunk_id += 1\n",
    "    parent_chunk_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a653421",
   "metadata": {},
   "source": [
    "## Contextual Chunking\n",
    "For each semantic chunk: gather p.text (parent), sc.text (semantic), then call LLM with prompt like:\n",
    "\n",
    "“Here is context from the parent section: {parent_text}. Now here is the semantic chunk: {sc_text}. Please produce an enriched chunk which retains the semantic chunk but adds any necessary context from the parent so that the chunk is self-standing.”\n",
    "\n",
    "Store output as contextual_chunk and save in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85da0f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Here is the parent section: {parent_text}. Now here is the semantic chunk: {sc_text}. \n",
    "Please produce an enriched chunk which retains the semantic chunk but adds any necessary context from the parent so that the chunk is self-standing.\n",
    "Do not add anything that is not needed to make the chunk self-standing.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55c4b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c3ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{os.getenv(\"BOOTCAMP_ROOT_DIR\")}/output\", exist_ok=True)\n",
    "contextual_chunks = []\n",
    "with open(f\"{os.getenv(\"BOOTCAMP_ROOT_DIR\")}/output/semantic_chunks.jsonl\", \"w\") as f:\n",
    "    for sc in semantic_chunks:\n",
    "        contextual_chunk_dict = {}\n",
    "        parent_text = sc[\"parent_chunk\"].text\n",
    "        sc_text = sc[\"chunk\"].text\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-oss:20b\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": system_prompt.format(parent_text=parent_text, sc_text=sc_text)},\n",
    "            ],\n",
    "        )\n",
    "        contextual_chunk_dict[\"contextual_chunk\"] = response.choices[0].message.content\n",
    "        contextual_chunk_dict[\"semantic_chunk_id\"] = sc[\"chunk_id\"]\n",
    "        contextual_chunk_dict[\"parent_id\"] = sc[\"parent_id\"]\n",
    "        contextual_chunk_dict[\"semantic_chunk\"] = sc[\"chunk\"].text\n",
    "        contextual_chunk_dict[\"parent_chunk\"] = sc[\"parent_chunk\"].text\n",
    "        contextual_chunks.append(contextual_chunk_dict)\n",
    "        f.write(json.dumps(contextual_chunk_dict) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd125139",
   "metadata": {},
   "source": [
    "## Late Chunking\n",
    "Alternatively, use the `Late Chunking` approach along lines of how it is explained in [Jina AI](https://github.com/jina-ai/late-chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "288e9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_name = \"jinaai/jina-embeddings-v2-base-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True, output_hidden_states=True)\n",
    "\n",
    "def late_chunk_parent(parent_chunk):\n",
    "    \"\"\"\n",
    "    parent_chunk: dict with keys {id, text, semantic_chunks: List[{start_char, end_char, text, id}]}\n",
    "    Returns same structure with added embeddings for each semantic chunk.\n",
    "    \"\"\"\n",
    "    text = parent_chunk[\"text\"]\n",
    "    sem_chunks = parent_chunk[\"semantic_chunks\"]\n",
    "\n",
    "    # Tokenize + embed the *parent chunk text only*\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=False, return_offsets_mapping=True)\n",
    "    # Save offsets separately\n",
    "    offsets = inputs.pop(\"offset_mapping\")[0].tolist()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    token_embs = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "    enriched_semantics = []\n",
    "    for sc in sem_chunks:\n",
    "        s, e = sc[\"start_char\"], sc[\"end_char\"]\n",
    "        indices = [i for i, (ts, te) in enumerate(offsets) if te > s and ts < e]\n",
    "        if not indices:\n",
    "            continue\n",
    "        emb = token_embs[indices].mean(dim=0).cpu().numpy()\n",
    "        enriched_semantics.append({\n",
    "            \"semantic_id\": sc[\"id\"],\n",
    "            \"parent_id\": parent_chunk[\"id\"],\n",
    "            \"embedding\": emb.tolist(),\n",
    "            \"text\": sc[\"text\"],\n",
    "            \"num_tokens\": len(indices)\n",
    "        })\n",
    "    return enriched_semantics\n",
    "\n",
    "# Example loop over all parent chunks\n",
    "all_embeddings = []\n",
    "os.makedirs(f\"{os.getenv(\"BOOTCAMP_ROOT_DIR\")}/output\", exist_ok=True)\n",
    "with open(f\"{os.getenv(\"BOOTCAMP_ROOT_DIR\")}/output/late_chunks.jsonl\", \"w\") as f:\n",
    "    for parent in parent_chunk_dict_list:   # parent_chunks from Docling\n",
    "        enriched_semantics = late_chunk_parent(parent)\n",
    "        all_embeddings.extend(enriched_semantics)\n",
    "        for es in enriched_semantics:\n",
    "            f.write(json.dumps(es) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
